"use strict";(globalThis.webpackChunkhumanoid_robotic_book=globalThis.webpackChunkhumanoid_robotic_book||[]).push([[448],{299:(e,n,t)=>{t.d(n,{A:()=>c});var o=t(6540);const a="interactiveLessonContainer_pdzt",i="lessonHeader_BiVh",r="lessonMeta_jdmH",s="lessonContent_Ivyb",c=({title:e,chapter:n,lesson:t,children:c})=>o.createElement("div",{className:a},o.createElement("div",{className:i},o.createElement("h1",null,e),o.createElement("div",{className:r},"Chapter ",n,", Lesson ",t)),o.createElement("div",{className:s},c))},4622:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>c,toc:()=>l});var o=t(8168),a=(t(6540),t(5680)),i=t(299);const r={sidebar_position:1},s=void 0,c={unversionedId:"chapter5/lesson1/voice-to-action",id:"chapter5/lesson1/voice-to-action",title:"voice-to-action",description:"In this comprehensive lesson, you'll explore how to implement voice-to-action systems for humanoid robots using advanced speech recognition technologies, including OpenAI's Whisper model. Voice-to-action systems enable natural human-robot interaction, allowing users to control robots through spoken commands in a conversational manner.",source:"@site/docs/chapter5/lesson1/voice-to-action.mdx",sourceDirName:"chapter5/lesson1",slug:"/chapter5/lesson1/voice-to-action",permalink:"/humanoid_robotic_book/docs/chapter5/lesson1/voice-to-action",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"mySidebar",previous:{title:"index",permalink:"/humanoid_robotic_book/docs/chapter5/"},next:{title:"cognitive-planning",permalink:"/humanoid_robotic_book/docs/chapter5/lesson2/cognitive-planning"}},d={},l=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"Key Components of Voice-to-Action Systems",id:"key-components-of-voice-to-action-systems",level:3},{value:"Speech Recognition with Whisper",id:"speech-recognition-with-whisper",level:2},{value:"Introduction to Whisper",id:"introduction-to-whisper",level:3},{value:"Whisper Integration in ROS 2",id:"whisper-integration-in-ros-2",level:3},{value:"Natural Language Understanding for Commands",id:"natural-language-understanding-for-commands",level:2},{value:"Command Parser and Intent Classifier",id:"command-parser-and-intent-classifier",level:3},{value:"Voice-to-Action Pipeline Integration",id:"voice-to-action-pipeline-integration",level:2},{value:"Complete Voice-to-Action System",id:"complete-voice-to-action-system",level:3},{value:"Advanced Voice Processing Techniques",id:"advanced-voice-processing-techniques",level:2},{value:"Voice Activity Detection and Noise Reduction",id:"voice-activity-detection-and-noise-reduction",level:3},{value:"Speech Synthesis for Feedback",id:"speech-synthesis-for-feedback",level:2},{value:"Text-to-Speech Integration",id:"text-to-speech-integration",level:3},{value:"Real-time Voice Processing",id:"real-time-voice-processing",level:2},{value:"Optimized Real-time Processing Pipeline",id:"optimized-real-time-processing-pipeline",level:3},{value:"Best Practices for Voice-to-Action Systems",id:"best-practices-for-voice-to-action-systems",level:2},{value:"1. Robustness and Error Handling",id:"1-robustness-and-error-handling",level:3},{value:"2. Performance Optimization",id:"2-performance-optimization",level:3},{value:"3. Privacy and Security",id:"3-privacy-and-security",level:3},{value:"4. User Experience",id:"4-user-experience",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Recognition Accuracy",id:"1-recognition-accuracy",level:3},{value:"2. Latency Issues",id:"2-latency-issues",level:3},{value:"3. Context Understanding",id:"3-context-understanding",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Quiz Questions",id:"quiz-questions",level:2}],m={toc:l},p="wrapper";function u({components:e,...n}){return(0,a.yg)(p,(0,o.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)(i.A,{title:"Voice-to-Action (Whisper Integration)",chapter:5,lesson:1,mdxType:"InteractiveLesson"},(0,a.yg)("h1",{id:"voice-to-action-whisper-integration"},"Voice-to-Action (Whisper Integration)"),(0,a.yg)("p",null,"In this comprehensive lesson, you'll explore how to implement voice-to-action systems for humanoid robots using advanced speech recognition technologies, including OpenAI's Whisper model. Voice-to-action systems enable natural human-robot interaction, allowing users to control robots through spoken commands in a conversational manner."),(0,a.yg)("h2",{id:"introduction-to-voice-to-action-systems"},"Introduction to Voice-to-Action Systems"),(0,a.yg)("p",null,"Voice-to-action systems bridge the gap between human language and robot actions by:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Speech Recognition"),": Converting spoken language to text"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Natural Language Understanding"),": Interpreting user intent"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Action Mapping"),": Translating commands to robot behaviors"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Feedback Generation"),": Providing confirmation and status updates")),(0,a.yg)("h3",{id:"key-components-of-voice-to-action-systems"},"Key Components of Voice-to-Action Systems"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Audio Input Processing"),": Capturing and preprocessing speech signals"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Speech Recognition Engine"),": Converting speech to text"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Intent Classification"),": Understanding command semantics"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Action Execution"),": Mapping commands to robot behaviors"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Voice Feedback"),": Providing auditory responses")),(0,a.yg)("h2",{id:"speech-recognition-with-whisper"},"Speech Recognition with Whisper"),(0,a.yg)("h3",{id:"introduction-to-whisper"},"Introduction to Whisper"),(0,a.yg)("p",null,"OpenAI's Whisper is a state-of-the-art automatic speech recognition (ASR) system trained on a large dataset of diverse audio. It offers several advantages for robotics applications:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Multilingual Support"),": Can recognize speech in multiple languages"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Robust Performance"),": Works well in various acoustic conditions"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Open Source"),": Free to use and modify"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Multiple Models"),": Various sizes for different performance/accuracy needs")),(0,a.yg)("h3",{id:"whisper-integration-in-ros-2"},"Whisper Integration in ROS 2"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport json\n\nclass WhisperSpeechRecognizer(Node):\n    def __init__(self):\n        super().__init__(\'whisper_speech_recognizer\')\n\n        # Initialize Whisper model\n        self.get_logger().info(\'Loading Whisper model...\')\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n        self.get_logger().info(\'Whisper model loaded successfully\')\n\n        # Audio processing parameters\n        self.sample_rate = 16000\n        self.chunk_size = 1024\n        self.audio_queue = queue.Queue()\n        self.recording = False\n\n        # Publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/audio_input\', self.audio_callback, 10)\n        self.text_pub = self.create_publisher(\n            String, \'/speech_text\', 10)\n\n        # Timer for processing audio chunks\n        self.process_timer = self.create_timer(1.0, self.process_audio)\n\n        # Audio stream for direct microphone input (optional)\n        self.audio_stream = None\n        self.setup_audio_stream()\n\n    def setup_audio_stream(self):\n        """Setup audio stream for microphone input"""\n        try:\n            self.audio = pyaudio.PyAudio()\n            self.audio_stream = self.audio.open(\n                format=pyaudio.paInt16,\n                channels=1,\n                rate=self.sample_rate,\n                input=True,\n                frames_per_buffer=self.chunk_size\n            )\n            self.get_logger().info(\'Audio stream initialized\')\n        except Exception as e:\n            self.get_logger().warn(f\'Could not initialize audio stream: {e}\')\n\n    def audio_callback(self, msg):\n        """Handle audio data from ROS topic"""\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n        self.audio_queue.put(audio_data)\n\n    def process_audio(self):\n        """Process accumulated audio data with Whisper"""\n        if self.audio_queue.empty():\n            return\n\n        # Collect audio chunks\n        audio_chunks = []\n        while not self.audio_queue.empty():\n            chunk = self.audio_queue.get()\n            audio_chunks.append(chunk)\n\n        if not audio_chunks:\n            return\n\n        # Concatenate all chunks\n        full_audio = np.concatenate(audio_chunks)\n\n        # Process with Whisper if we have enough audio (at least 1 second)\n        if len(full_audio) >= self.sample_rate:\n            self.recognize_speech(full_audio)\n\n    def recognize_speech(self, audio_data):\n        """Recognize speech using Whisper model"""\n        try:\n            # Convert to the format expected by Whisper\n            audio_tensor = torch.from_numpy(audio_data).float()\n\n            # Transcribe the audio\n            result = self.model.transcribe(audio_tensor.numpy())\n\n            # Extract text and confidence\n            text = result[\'text\'].strip()\n            if text:  # Only publish if we have text\n                self.get_logger().info(f\'Recognized: {text}\')\n\n                # Publish recognized text\n                text_msg = String()\n                text_msg.data = text\n                self.text_pub.publish(text_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in speech recognition: {e}\')\n\n    def record_audio_continuously(self):\n        """Continuously record audio from microphone (separate thread)"""\n        def recording_thread():\n            while self.recording:\n                try:\n                    data = self.audio_stream.read(self.chunk_size, exception_on_overflow=False)\n                    audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n                    self.audio_queue.put(audio_data)\n                except Exception as e:\n                    self.get_logger().error(f\'Error in audio recording: {e}\')\n\n        self.recording = True\n        thread = threading.Thread(target=recording_thread, daemon=True)\n        thread.start()\n\n    def stop_recording(self):\n        """Stop audio recording"""\n        self.recording = False\n        if self.audio_stream:\n            self.audio_stream.stop_stream()\n            self.audio_stream.close()\n        if self.audio:\n            self.audio.terminate()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    recognizer = WhisperSpeechRecognizer()\n\n    try:\n        # Optionally start continuous recording\n        recognizer.record_audio_continuously()\n        rclpy.spin(recognizer)\n    except KeyboardInterrupt:\n        recognizer.get_logger().info(\'Shutting down\')\n    finally:\n        recognizer.stop_recording()\n        recognizer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,a.yg)("h2",{id:"natural-language-understanding-for-commands"},"Natural Language Understanding for Commands"),(0,a.yg)("h3",{id:"command-parser-and-intent-classifier"},"Command Parser and Intent Classifier"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import re\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport spacy\n\nclass CommandType(Enum):\n    MOVE = "move"\n    GREET = "greet"\n    FOLLOW = "follow"\n    STOP = "stop"\n    DANCE = "dance"\n    FETCH = "fetch"\n    SPEAK = "speak"\n    NAVIGATE = "navigate"\n    UNKNOWN = "unknown"\n\n@dataclass\nclass ParsedCommand:\n    command_type: CommandType\n    parameters: Dict[str, str]\n    confidence: float\n    original_text: str\n\nclass CommandParser:\n    def __init__(self):\n        # Load spaCy model for NLP processing\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            self.get_logger().warn("spaCy model not found. Install with: python -m spacy download en_core_web_sm")\n            self.nlp = None\n\n        # Define command patterns\n        self.command_patterns = {\n            CommandType.MOVE: [\n                r"go\\s+(?P<direction>\\w+)",\n                r"move\\s+(?P<direction>\\w+)",\n                r"walk\\s+(?P<direction>\\w+)",\n                r"step\\s+(?P<direction>\\w+)"\n            ],\n            CommandType.GREET: [\n                r"hello",\n                r"hi",\n                r"greet",\n                r"say\\s+hello",\n                r"introduce\\s+yourself"\n            ],\n            CommandType.FOLLOW: [\n                r"follow\\s+(?P<target>\\w+)",\n                r"come\\s+with\\s+me",\n                r"follow\\s+me"\n            ],\n            CommandType.STOP: [\n                r"stop",\n                r"halt",\n                r"freeze",\n                r"stand\\s+still"\n            ],\n            CommandType.DANCE: [\n                r"dance",\n                r"boogie",\n                r"move\\s+to\\s+music"\n            ],\n            CommandType.FETCH: [\n                r"fetch\\s+(?P<object>\\w+)",\n                r"get\\s+(?P<object>\\w+)",\n                r"bring\\s+(?P<object>\\w+)",\n                r"pick\\s+up\\s+(?P<object>\\w+)"\n            ],\n            CommandType.NAVIGATE: [\n                r"go\\s+to\\s+(?P<location>[\\w\\s]+)",\n                r"navigate\\s+to\\s+(?P<location>[\\w\\s]+)",\n                r"move\\s+to\\s+(?P<location>[\\w\\s]+)"\n            ]\n        }\n\n    def parse_command(self, text: str) -> ParsedCommand:\n        """Parse text command and extract intent and parameters"""\n        text_lower = text.lower().strip()\n\n        # Try to match against known patterns\n        for cmd_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    return ParsedCommand(\n                        command_type=cmd_type,\n                        parameters=match.groupdict(),\n                        confidence=0.9,  # High confidence for pattern matches\n                        original_text=text\n                    )\n\n        # If no pattern matches, try NLP-based classification\n        return self.nlp_classify_command(text)\n\n    def nlp_classify_command(self, text: str) -> ParsedCommand:\n        """Use NLP to classify command when pattern matching fails"""\n        if not self.nlp:\n            return ParsedCommand(\n                command_type=CommandType.UNKNOWN,\n                parameters={},\n                confidence=0.0,\n                original_text=text\n            )\n\n        doc = self.nlp(text)\n\n        # Extract entities and intent based on keywords\n        entities = {ent.label_: ent.text for ent in doc.ents}\n        tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n\n        # Simple keyword-based classification\n        if any(word in tokens for word in [\'go\', \'move\', \'walk\', \'step\']):\n            return ParsedCommand(\n                command_type=CommandType.MOVE,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'hello\', \'hi\', \'greet\']):\n            return ParsedCommand(\n                command_type=CommandType.GREET,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'stop\', \'halt\']):\n            return ParsedCommand(\n                command_type=CommandType.STOP,\n                parameters=entities,\n                confidence=0.8,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'follow\', \'come\']):\n            return ParsedCommand(\n                command_type=CommandType.FOLLOW,\n                parameters=entities,\n                confidence=0.75,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'dance\', \'boogie\']):\n            return ParsedCommand(\n                command_type=CommandType.DANCE,\n                parameters=entities,\n                confidence=0.8,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'fetch\', \'get\', \'bring\', \'pick\']):\n            return ParsedCommand(\n                command_type=CommandType.FETCH,\n                parameters=entities,\n                confidence=0.75,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'navigate\', \'go\', \'move\', \'to\']):\n            return ParsedCommand(\n                command_type=CommandType.NAVIGATE,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n\n        return ParsedCommand(\n            command_type=CommandType.UNKNOWN,\n            parameters=entities,\n            confidence=0.3,\n            original_text=text\n        )\n\n    def extract_parameters(self, text: str, cmd_type: CommandType) -> Dict[str, str]:\n        """Extract specific parameters for command type"""\n        params = {}\n        text_lower = text.lower()\n\n        if cmd_type == CommandType.MOVE:\n            # Extract direction\n            directions = [\'forward\', \'backward\', \'left\', \'right\', \'up\', \'down\']\n            for direction in directions:\n                if direction in text_lower:\n                    params[\'direction\'] = direction\n                    break\n\n        elif cmd_type == CommandType.NAVIGATE:\n            # Extract destination\n            match = re.search(r\'to\\s+(.+?)(?:\\.|$)\', text_lower)\n            if match:\n                params[\'destination\'] = match.group(1).strip()\n\n        elif cmd_type == CommandType.FETCH:\n            # Extract object to fetch\n            match = re.search(r\'(?:fetch|get|bring|pick\\s+up)\\s+(.+?)(?:\\.|$)\', text_lower)\n            if match:\n                params[\'object\'] = match.group(1).strip()\n\n        return params\n')),(0,a.yg)("h2",{id:"voice-to-action-pipeline-integration"},"Voice-to-Action Pipeline Integration"),(0,a.yg)("h3",{id:"complete-voice-to-action-system"},"Complete Voice-to-Action System"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom humanoid_robot_msgs.msg import RobotCommand\nfrom builtin_interfaces.msg import Duration\nimport time\n\nclass VoiceToActionSystem(Node):\n    def __init__(self):\n        super().__init__(\'voice_to_action_system\')\n\n        # Initialize components\n        self.command_parser = CommandParser()\n        self.action_executor = ActionExecutor(self)\n\n        # Publishers and subscribers\n        self.speech_sub = self.create_subscription(\n            String, \'/speech_text\', self.speech_callback, 10)\n        self.robot_cmd_pub = self.create_publisher(\n            RobotCommand, \'/robot_command\', 10)\n        self.velocity_pub = self.create_publisher(\n            Twist, \'/cmd_vel\', 10)\n\n        # Feedback publisher\n        self.feedback_pub = self.create_publisher(\n            String, \'/voice_feedback\', 10)\n\n        self.get_logger().info(\'Voice-to-Action system initialized\')\n\n    def speech_callback(self, msg):\n        """Process incoming speech text"""\n        text = msg.data.strip()\n        if not text:\n            return\n\n        self.get_logger().info(f\'Received speech command: {text}\')\n\n        # Parse the command\n        parsed_command = self.command_parser.parse_command(text)\n\n        if parsed_command.command_type != CommandType.UNKNOWN:\n            self.get_logger().info(f\'Parsed command: {parsed_command.command_type.value}, \'\n                                 f\'Parameters: {parsed_command.parameters}, \'\n                                 f\'Confidence: {parsed_command.confidence:.2f}\')\n\n            # Execute the command\n            success = self.action_executor.execute_command(parsed_command)\n\n            # Provide feedback\n            if success:\n                feedback = f"Executing {parsed_command.command_type.value} command"\n            else:\n                feedback = f"Failed to execute {parsed_command.command_type.value} command"\n\n            self.provide_feedback(feedback)\n        else:\n            self.get_logger().warn(f\'Unknown command: {text}\')\n            self.provide_feedback("I didn\'t understand that command")\n\n    def provide_feedback(self, message: str):\n        """Provide auditory or visual feedback"""\n        feedback_msg = String()\n        feedback_msg.data = message\n        self.feedback_pub.publish(feedback_msg)\n\n        self.get_logger().info(f\'Feedback: {message}\')\n\nclass ActionExecutor:\n    def __init__(self, node: Node):\n        self.node = node\n        self.current_behavior = None\n\n    def execute_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute the parsed command"""\n        try:\n            if parsed_command.command_type == CommandType.MOVE:\n                return self.execute_move_command(parsed_command)\n            elif parsed_command.command_type == CommandType.GREET:\n                return self.execute_greet_command(parsed_command)\n            elif parsed_command.command_type == CommandType.FOLLOW:\n                return self.execute_follow_command(parsed_command)\n            elif parsed_command.command_type == CommandType.STOP:\n                return self.execute_stop_command(parsed_command)\n            elif parsed_command.command_type == CommandType.DANCE:\n                return self.execute_dance_command(parsed_command)\n            elif parsed_command.command_type == CommandType.FETCH:\n                return self.execute_fetch_command(parsed_command)\n            elif parsed_command.command_type == CommandType.NAVIGATE:\n                return self.execute_navigate_command(parsed_command)\n            else:\n                self.node.get_logger().warn(f\'Unsupported command type: {parsed_command.command_type}\')\n                return False\n\n        except Exception as e:\n            self.node.get_logger().error(f\'Error executing command: {e}\')\n            return False\n\n    def execute_move_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute move command"""\n        direction = parsed_command.parameters.get(\'direction\', \'forward\')\n        distance = float(parsed_command.parameters.get(\'distance\', 1.0))  # Default 1 meter\n\n        # Create velocity command based on direction\n        twist = Twist()\n\n        if direction in [\'forward\', \'ahead\']:\n            twist.linear.x = 0.5  # m/s\n        elif direction in [\'backward\', \'back\']:\n            twist.linear.x = -0.5\n        elif direction in [\'left\']:\n            twist.linear.y = 0.5\n        elif direction in [\'right\']:\n            twist.linear.y = -0.5\n        elif direction in [\'up\']:\n            twist.linear.z = 0.5\n        elif direction in [\'down\']:\n            twist.linear.z = -0.5\n\n        # Publish command for duration based on distance\n        duration = Duration()\n        duration.sec = int(distance / 0.5)  # Assuming 0.5 m/s speed\n        duration.nanosec = int((distance / 0.5 - duration.sec) * 1e9)\n\n        # Publish the command\n        self.node.velocity_pub.publish(twist)\n\n        # Stop after the specified duration\n        time.sleep(distance / 0.5)\n\n        # Stop the robot\n        stop_twist = Twist()\n        self.node.velocity_pub.publish(stop_twist)\n\n        return True\n\n    def execute_greet_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute greeting command"""\n        # This would trigger a greeting behavior\n        # Could involve speech synthesis, gestures, etc.\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "GREET"\n        robot_cmd.parameters = json.dumps({"greeting_type": "wave_hello"})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_follow_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute follow command"""\n        target = parsed_command.parameters.get(\'target\', \'me\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "FOLLOW"\n        robot_cmd.parameters = json.dumps({"target": target})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_stop_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute stop command"""\n        # Stop any ongoing movement\n        stop_twist = Twist()\n        self.node.velocity_pub.publish(stop_twist)\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "STOP"\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_dance_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute dance command"""\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "DANCE"\n        robot_cmd.parameters = json.dumps({"dance_type": "basic"})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_fetch_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute fetch command"""\n        obj = parsed_command.parameters.get(\'object\', \'item\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "FETCH"\n        robot_cmd.parameters = json.dumps({"object": obj})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_navigate_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute navigation command"""\n        destination = parsed_command.parameters.get(\'destination\', \'here\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "NAVIGATE"\n        robot_cmd.parameters = json.dumps({"destination": destination})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_system = VoiceToActionSystem()\n\n    try:\n        rclpy.spin(voice_system)\n    except KeyboardInterrupt:\n        voice_system.get_logger().info(\'Shutting down Voice-to-Action system\')\n    finally:\n        voice_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,a.yg)("h2",{id:"advanced-voice-processing-techniques"},"Advanced Voice Processing Techniques"),(0,a.yg)("h3",{id:"voice-activity-detection-and-noise-reduction"},"Voice Activity Detection and Noise Reduction"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import numpy as np\nimport webrtcvad\nfrom scipy import signal\nimport collections\n\nclass AdvancedVoiceProcessor:\n    def __init__(self):\n        # Initialize WebRTC VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(3)  # Aggressive mode\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Circular buffer for audio frames\n        self.audio_buffer = collections.deque(maxlen=100)  # Store last 100 frames\n\n        # Noise reduction parameters\n        self.noise_threshold = 0.01\n        self.speech_threshold = 0.1\n\n    def detect_voice_activity(self, audio_frame):\n        """Detect if the audio frame contains speech"""\n        # Convert to bytes for WebRTC VAD\n        audio_bytes = (audio_frame * 32767).astype(np.int16).tobytes()\n\n        try:\n            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)\n            return is_speech\n        except:\n            # Fallback: simple energy-based detection\n            energy = np.mean(np.abs(audio_frame))\n            return energy > self.speech_threshold\n\n    def reduce_noise(self, audio_data):\n        """Apply basic noise reduction"""\n        # Apply a simple high-pass filter to remove low-frequency noise\n        b, a = signal.butter(4, 100 / (self.sample_rate / 2), btype=\'high\')\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n\n        # Apply spectral subtraction for noise reduction\n        # (Simplified version - real implementation would be more complex)\n        noise_profile = self.estimate_noise_profile(audio_data)\n        enhanced_audio = self.spectral_subtraction(audio_data, noise_profile)\n\n        return enhanced_audio\n\n    def estimate_noise_profile(self, audio_data):\n        """Estimate noise profile from silent segments"""\n        # For simplicity, assume first 10% of audio is noise\n        noise_segment = audio_data[:len(audio_data)//10]\n        noise_profile = np.mean(np.abs(noise_segment))\n        return noise_profile\n\n    def spectral_subtraction(self, audio_data, noise_profile):\n        """Apply spectral subtraction for noise reduction"""\n        # Convert to frequency domain\n        fft_data = np.fft.fft(audio_data)\n        magnitude = np.abs(fft_data)\n\n        # Subtract noise profile\n        enhanced_magnitude = np.maximum(magnitude - noise_profile, 0)\n\n        # Reconstruct signal\n        phase = np.angle(fft_data)\n        enhanced_fft = enhanced_magnitude * np.exp(1j * phase)\n        enhanced_audio = np.real(np.fft.ifft(enhanced_fft))\n\n        return enhanced_audio\n\n    def preprocess_audio(self, raw_audio):\n        """Complete audio preprocessing pipeline"""\n        # Apply noise reduction\n        denoised_audio = self.reduce_noise(raw_audio)\n\n        # Normalize audio\n        normalized_audio = self.normalize_audio(denoised_audio)\n\n        # Apply voice activity detection\n        is_speech = self.detect_voice_activity(normalized_audio[:self.frame_size])\n\n        return normalized_audio, is_speech\n\n    def normalize_audio(self, audio_data):\n        """Normalize audio to standard range"""\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            normalized = audio_data / max_val\n            # Scale to reasonable range (not too quiet, not clipping)\n            normalized = normalized * 0.8\n        else:\n            normalized = audio_data\n\n        return normalized\n')),(0,a.yg)("h2",{id:"speech-synthesis-for-feedback"},"Speech Synthesis for Feedback"),(0,a.yg)("h3",{id:"text-to-speech-integration"},"Text-to-Speech Integration"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import pyttsx3\nimport threading\nfrom queue import Queue\n\nclass TextToSpeech:\n    def __init__(self):\n        self.tts_engine = pyttsx3.init()\n\n        # Configure speech properties\n        self.tts_engine.setProperty(\'rate\', 150)  # Words per minute\n        self.tts_engine.setProperty(\'volume\', 0.8)  # Volume level (0.0 to 1.0)\n\n        # Get available voices\n        voices = self.tts_engine.getProperty(\'voices\')\n        if voices:\n            # Use the first available voice (typically female)\n            self.tts_engine.setProperty(\'voice\', voices[0].id)\n\n        # Queue for speech requests\n        self.speech_queue = Queue()\n        self.speaking = False\n\n        # Start speech processing thread\n        self.speech_thread = threading.Thread(target=self.speech_worker, daemon=True)\n        self.speech_thread.start()\n\n    def speak(self, text):\n        """Add text to speech queue"""\n        self.speech_queue.put(text)\n\n    def speech_worker(self):\n        """Process speech requests in separate thread"""\n        while True:\n            try:\n                text = self.speech_queue.get(timeout=1.0)\n                if text:\n                    self.tts_engine.say(text)\n                    self.tts_engine.runAndWait()\n            except:\n                continue  # Timeout, continue loop\n\n    def interrupt_speech(self):\n        """Interrupt current speech"""\n        self.tts_engine.stop()\n\nclass VoiceFeedbackSystem:\n    def __init__(self, node):\n        self.node = node\n        self.tts = TextToSpeech()\n\n        # Predefined responses\n        self.responses = {\n            \'acknowledged\': [\n                "I understand",\n                "Got it",\n                "Okay",\n                "Understood"\n            ],\n            \'executing\': [\n                "I\'m executing that command",\n                "Working on it now",\n                "On it",\n                "Carrying out your request"\n            ],\n            \'completed\': [\n                "Command completed",\n                "Done",\n                "Finished",\n                "All set"\n            ],\n            \'error\': [\n                "I encountered an error",\n                "Something went wrong",\n                "I couldn\'t complete that",\n                "Error occurred"\n            ]\n        }\n\n    def provide_feedback(self, message_type, details=""):\n        """Provide voice feedback based on message type"""\n        import random\n\n        if message_type in self.responses:\n            response = random.choice(self.responses[message_type])\n            if details:\n                response += f". {details}"\n        else:\n            response = str(message_type)\n\n        self.node.get_logger().info(f\'Voice feedback: {response}\')\n        self.tts.speak(response)\n')),(0,a.yg)("h2",{id:"real-time-voice-processing"},"Real-time Voice Processing"),(0,a.yg)("h3",{id:"optimized-real-time-processing-pipeline"},"Optimized Real-time Processing Pipeline"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport threading\nimport queue\nimport time\nimport numpy as np\n\nclass RealTimeVoiceProcessor(Node):\n    def __init__(self):\n        super().__init__(\'real_time_voice_processor\')\n\n        # Initialize components\n        self.whisper_model = whisper.load_model("base")\n        self.command_parser = CommandParser()\n        self.voice_feedback = VoiceFeedbackSystem(self)\n\n        # Audio processing\n        self.audio_queue = queue.Queue(maxsize=10)\n        self.processing_lock = threading.Lock()\n        self.last_process_time = time.time()\n\n        # Publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/microphone/audio_raw\', self.audio_callback, 10)\n        self.command_pub = self.create_publisher(\n            String, \'/parsed_commands\', 10)\n\n        # Processing timer\n        self.process_timer = self.create_timer(0.1, self.process_audio_queue)\n\n        # Performance metrics\n        self.processed_audio_count = 0\n        self.start_time = time.time()\n\n    def audio_callback(self, msg):\n        """Handle incoming audio data"""\n        try:\n            # Convert audio data\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to processing queue\n            if not self.audio_queue.full():\n                self.audio_queue.put(audio_data)\n            else:\n                # Queue is full, drop oldest\n                try:\n                    self.audio_queue.get_nowait()\n                    self.audio_queue.put(audio_data)\n                except:\n                    pass  # Queue might be empty now\n        except Exception as e:\n            self.get_logger().error(f\'Error in audio callback: {e}\')\n\n    def process_audio_queue(self):\n        """Process accumulated audio in real-time"""\n        if self.audio_queue.empty():\n            return\n\n        # Collect available audio data\n        audio_chunks = []\n        while not self.audio_queue.empty():\n            try:\n                chunk = self.audio_queue.get_nowait()\n                audio_chunks.append(chunk)\n            except:\n                break\n\n        if not audio_chunks:\n            return\n\n        # Concatenate chunks\n        full_audio = np.concatenate(audio_chunks)\n\n        # Only process if we have enough audio and enough time has passed\n        current_time = time.time()\n        if len(full_audio) >= 8000 and (current_time - self.last_process_time) > 1.0:  # At least 0.5 seconds of audio\n            with self.processing_lock:\n                # Process with Whisper\n                self.process_audio_with_whisper(full_audio)\n                self.last_process_time = current_time\n\n    def process_audio_with_whisper(self, audio_data):\n        """Process audio with Whisper model"""\n        try:\n            # Transcribe audio\n            result = self.whisper_model.transcribe(audio_data)\n            text = result[\'text\'].strip()\n\n            if text:\n                self.get_logger().info(f\'Recognized: {text}\')\n\n                # Parse command\n                parsed_command = self.command_parser.parse_command(text)\n\n                if parsed_command.command_type != CommandType.UNKNOWN:\n                    # Publish parsed command\n                    cmd_msg = String()\n                    cmd_msg.data = f"{parsed_command.command_type.value}:{parsed_command.parameters}"\n                    self.command_pub.publish(cmd_msg)\n\n                    # Provide feedback\n                    self.voice_feedback.provide_feedback(\'acknowledged\')\n\n                    # Execute command (this would be handled by another node)\n                    self.execute_robot_command(parsed_command)\n                else:\n                    self.voice_feedback.provide_feedback(\'error\', "Command not understood")\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in Whisper processing: {e}\')\n\n    def execute_robot_command(self, parsed_command):\n        """Execute robot command (placeholder - would interface with robot control)"""\n        # This would send the command to the robot\'s action execution system\n        self.get_logger().info(f\'Executing command: {parsed_command.command_type.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = RealTimeVoiceProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info(\'Shutting down real-time voice processor\')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,a.yg)("h2",{id:"best-practices-for-voice-to-action-systems"},"Best Practices for Voice-to-Action Systems"),(0,a.yg)("h3",{id:"1-robustness-and-error-handling"},"1. Robustness and Error Handling"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Implement multiple fallback strategies"),(0,a.yg)("li",{parentName:"ul"},"Handle network interruptions gracefully"),(0,a.yg)("li",{parentName:"ul"},"Provide clear error feedback to users"),(0,a.yg)("li",{parentName:"ul"},"Design for various acoustic environments")),(0,a.yg)("h3",{id:"2-performance-optimization"},"2. Performance Optimization"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Use appropriate model sizes for real-time requirements"),(0,a.yg)("li",{parentName:"ul"},"Implement audio buffering and streaming"),(0,a.yg)("li",{parentName:"ul"},"Optimize for minimal latency"),(0,a.yg)("li",{parentName:"ul"},"Consider edge computing for sensitive applications")),(0,a.yg)("h3",{id:"3-privacy-and-security"},"3. Privacy and Security"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Implement local processing when possible"),(0,a.yg)("li",{parentName:"ul"},"Secure voice data transmission"),(0,a.yg)("li",{parentName:"ul"},"Provide clear privacy controls"),(0,a.yg)("li",{parentName:"ul"},"Consider data retention policies")),(0,a.yg)("h3",{id:"4-user-experience"},"4. User Experience"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Design natural, conversational interactions"),(0,a.yg)("li",{parentName:"ul"},"Provide clear feedback for all actions"),(0,a.yg)("li",{parentName:"ul"},"Support multiple languages and accents"),(0,a.yg)("li",{parentName:"ul"},"Implement context-aware command understanding")),(0,a.yg)("h2",{id:"troubleshooting-common-issues"},"Troubleshooting Common Issues"),(0,a.yg)("h3",{id:"1-recognition-accuracy"},"1. Recognition Accuracy"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Ensure proper microphone placement and quality"),(0,a.yg)("li",{parentName:"ul"},"Use noise reduction techniques"),(0,a.yg)("li",{parentName:"ul"},"Train custom models for domain-specific vocabulary"),(0,a.yg)("li",{parentName:"ul"},"Implement confidence thresholds")),(0,a.yg)("h3",{id:"2-latency-issues"},"2. Latency Issues"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Optimize model size and complexity"),(0,a.yg)("li",{parentName:"ul"},"Use streaming recognition when possible"),(0,a.yg)("li",{parentName:"ul"},"Implement efficient audio buffering"),(0,a.yg)("li",{parentName:"ul"},"Consider hardware acceleration")),(0,a.yg)("h3",{id:"3-context-understanding"},"3. Context Understanding"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Implement conversation history tracking"),(0,a.yg)("li",{parentName:"ul"},"Use context-aware NLP models"),(0,a.yg)("li",{parentName:"ul"},"Design clear command structures"),(0,a.yg)("li",{parentName:"ul"},"Provide feedback on understood context")),(0,a.yg)("h2",{id:"hands-on-exercise"},"Hands-on Exercise"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Set up Whisper model for speech recognition"),(0,a.yg)("li",{parentName:"ol"},"Implement a command parser for robot actions"),(0,a.yg)("li",{parentName:"ol"},"Create a complete voice-to-action pipeline"),(0,a.yg)("li",{parentName:"ol"},"Test with various voice commands and environments"),(0,a.yg)("li",{parentName:"ol"},"Optimize for real-time performance and accuracy")),(0,a.yg)("h2",{id:"quiz-questions"},"Quiz Questions"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"What are the key components of a voice-to-action system for humanoid robots?"),(0,a.yg)("li",{parentName:"ol"},"How does Whisper improve speech recognition accuracy compared to traditional ASR systems?"),(0,a.yg)("li",{parentName:"ol"},"What are the main challenges in implementing real-time voice processing for robotics?"))))}u.isMDXComponent=!0},5680:(e,n,t)=>{t.d(n,{xA:()=>l,yg:()=>f});var o=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=o.createContext({}),d=function(e){var n=o.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},l=function(e){var n=d(e.components);return o.createElement(c.Provider,{value:n},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef(function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),m=d(t),u=a,f=m["".concat(c,".").concat(u)]||m[u]||p[u]||i;return t?o.createElement(f,r(r({ref:n},l),{},{components:t})):o.createElement(f,r({ref:n},l))});function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,r=new Array(i);r[0]=u;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s[m]="string"==typeof e?e:a,r[1]=s;for(var d=2;d<i;d++)r[d]=t[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"}}]);