---
sidebar_position: 1
---

import InteractiveLesson from '@site/src/components/InteractiveLesson';

<InteractiveLesson title="Bridging AI Agents (rclpy)" chapter={2} lesson={3}>

# Bridging AI Agents (rclpy)

In this comprehensive lesson, you'll explore how to integrate AI agents with ROS 2 using Python (rclpy), creating intelligent humanoid robots that can perceive, reason, and act in complex environments. This integration is fundamental to creating autonomous humanoid robots with advanced cognitive capabilities.

## Introduction to AI-ROS Integration with rclpy

The integration of AI agents with ROS 2 using Python (rclpy) creates a powerful framework for intelligent robotics. Python is particularly well-suited for AI development due to its rich ecosystem of machine learning libraries. This combination enables humanoid robots to:

- Process sensory information using machine learning models (TensorFlow, PyTorch, scikit-learn)
- Make intelligent decisions based on environmental context
- Execute complex behaviors through ROS 2 control systems
- Learn from experience and adapt to new situations

### Key Components of AI-ROS Integration

1. **Perception Systems**: AI models for vision, speech, and sensor processing
2. **Cognitive Systems**: Planning, reasoning, and decision-making modules
3. **Action Systems**: ROS 2 nodes for controlling robot actuators
4. **Learning Systems**: Models that adapt based on experience

## AI Agent Architecture with rclpy

### Perception Pipeline

The perception pipeline processes raw sensor data to extract meaningful information for decision-making using rclpy:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2
from std_msgs.msg import String
import cv2
from cv_bridge import CvBridge
import numpy as np

class PerceptionNode(Node):
    def __init__(self):
        super().__init__('perception_node')
        self.bridge = CvBridge()

        # Subscribe to camera and sensor topics
        self.image_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.image_callback, 10)
        self.pointcloud_sub = self.create_subscription(
            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10)

        # Publish processed information
        self.object_pub = self.create_publisher(String, '/detected_objects', 10)

        # Load AI models (placeholder - in practice, you'd load actual models)
        self.object_detector = self.load_object_detection_model()
        self.pose_estimator = self.load_pose_estimation_model()

        self.get_logger().info('Perception node initialized')

    def image_callback(self, msg):
        try:
            # Convert ROS image to OpenCV format
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Run object detection
            detections = self.object_detector(cv_image)

            # Process and publish results
            detection_msg = String()
            detection_msg.data = str(detections)
            self.object_pub.publish(detection_msg)

            self.get_logger().info(f'Detected objects: {detections}')
        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def pointcloud_callback(self, msg):
        # Process point cloud data for 3D understanding
        self.get_logger().info('Processing point cloud data')

    def load_object_detection_model(self):
        # Placeholder for loading an actual model
        # In practice, you might load YOLO, SSD, or other models
        def dummy_detector(image):
            # This would be replaced with actual model inference
            return {'objects': ['person', 'chair'], 'confidence': [0.9, 0.8]}
        return dummy_detector

    def load_pose_estimation_model(self):
        # Placeholder for pose estimation model
        def dummy_pose_estimator(image):
            return {'poses': []}
        return dummy_pose_estimator

def main(args=None):
    rclpy.init(args=args)
    perception_node = PerceptionNode()

    try:
        rclpy.spin(perception_node)
    except KeyboardInterrupt:
        perception_node.get_logger().info('Interrupted by user')
    finally:
        perception_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Cognitive Architecture with rclpy

The cognitive architecture orchestrates AI processing and decision-making using rclpy:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from humanoid_robot_msgs.msg import CognitiveState, ActionPlan
import json
from collections import defaultdict

class CognitiveNode(Node):
    def __init__(self):
        super().__init__('cognitive_node')

        # Subscriptions for sensory input
        self.perception_sub = self.create_subscription(
            String, '/detected_objects', self.perception_callback, 10)
        self.speech_sub = self.create_subscription(
            String, '/speech_recognition', self.speech_callback, 10)

        # Publishers for action plans
        self.plan_pub = self.create_publisher(ActionPlan, '/action_plan', 10)
        self.state_pub = self.create_publisher(CognitiveState, '/cognitive_state', 10)

        # Initialize AI components
        self.reasoning_engine = self.initialize_reasoning_engine()
        self.memory_system = self.initialize_memory_system()
        self.planning_system = self.initialize_planning_system()

        # Context variables
        self.current_context = {}
        self.memory_buffer = defaultdict(list)

        self.get_logger().info('Cognitive node initialized')

    def perception_callback(self, msg):
        try:
            # Process perception data
            perception_data = json.loads(msg.data) if msg.data.startswith('{') else {'raw_data': msg.data}

            # Update memory with new information
            self.memory_system.update(perception_data)

            # Reason about the current situation
            situation_assessment = self.reasoning_engine.assess_situation(
                perception_data, self.memory_system.get_context())

            # Generate action plan based on assessment
            action_plan = self.planning_system.generate_plan(situation_assessment)

            # Publish the plan
            plan_msg = ActionPlan()
            plan_msg.plan = json.dumps(action_plan)
            plan_msg.priority = 1  # Normal priority
            plan_msg.timestamp = self.get_clock().now().to_msg()
            self.plan_pub.publish(plan_msg)

            # Update and publish cognitive state
            cognitive_state = CognitiveState()
            cognitive_state.state = json.dumps(situation_assessment)
            cognitive_state.timestamp = self.get_clock().now().to_msg()
            self.state_pub.publish(cognitive_state)

        except Exception as e:
            self.get_logger().error(f'Error in perception callback: {e}')

    def speech_callback(self, msg):
        try:
            # Process speech commands
            command = msg.data.lower()

            if 'hello' in command or 'hi' in command:
                self.execute_greeting_behavior()
            elif 'dance' in command:
                self.execute_dance_behavior()
            elif 'follow' in command:
                self.execute_follow_behavior()

        except Exception as e:
            self.get_logger().error(f'Error processing speech: {e}')

    def execute_greeting_behavior(self):
        # Create a greeting action plan
        plan = {
            'action': 'greet',
            'sequence': [
                {'type': 'speak', 'text': 'Hello! Nice to meet you!'},
                {'type': 'gesture', 'name': 'wave'}
            ]
        }

        plan_msg = ActionPlan()
        plan_msg.plan = json.dumps(plan)
        plan_msg.priority = 2  # High priority for greeting
        self.plan_pub.publish(plan_msg)

    def initialize_reasoning_engine(self):
        return ReasoningEngine(self)

    def initialize_memory_system(self):
        return MemorySystem(self)

    def initialize_planning_system(self):
        return PlanningSystem(self)

class ReasoningEngine:
    def __init__(self, node):
        self.node = node

    def assess_situation(self, perception_data, context):
        # Simple reasoning based on perception data
        assessment = {
            'detected_objects': perception_data.get('objects', []),
            'confidence': perception_data.get('confidence', []),
            'context': context,
            'recommendation': self.make_recommendation(perception_data)
        }
        return assessment

    def make_recommendation(self, perception_data):
        objects = perception_data.get('objects', [])

        if 'person' in objects:
            return 'engage_with_human'
        elif 'obstacle' in objects:
            return 'avoid_obstacle'
        else:
            return 'explore_environment'

class MemorySystem:
    def __init__(self, node):
        self.node = node
        self.episodic_memory = []
        self.semantic_memory = {}

    def update(self, data):
        # Add new information to memory
        self.episodic_memory.append({
            'timestamp': self.node.get_clock().now().nanoseconds,
            'data': data
        })

    def get_context(self):
        # Return relevant context for current situation
        return {
            'recent_events': self.episodic_memory[-5:],  # Last 5 events
            'learned_patterns': self.semantic_memory
        }

class PlanningSystem:
    def __init__(self, node):
        self.node = node

    def generate_plan(self, assessment):
        # Generate an action plan based on assessment
        recommendation = assessment['recommendation']

        if recommendation == 'engage_with_human':
            return {
                'action': 'approach_human',
                'steps': [
                    {'move': 'towards_person'},
                    {'wait': 2.0},
                    {'greet': True}
                ]
            }
        elif recommendation == 'avoid_obstacle':
            return {
                'action': 'navigate_around',
                'steps': [
                    {'stop': True},
                    {'plan_route': 'around_obstacle'},
                    {'move': 'new_route'}
                ]
            }
        else:
            return {
                'action': 'explore',
                'steps': [
                    {'move': 'forward'},
                    {'turn': 'random_direction'}
                ]
            }

def main(args=None):
    rclpy.init(args=args)
    cognitive_node = CognitiveNode()

    try:
        rclpy.spin(cognitive_node)
    except KeyboardInterrupt:
        cognitive_node.get_logger().info('Interrupted by user')
    finally:
        cognitive_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Deep Learning Integration with rclpy

### TensorFlow/PyTorch Integration

Integrating deep learning frameworks with ROS 2 using rclpy requires careful consideration of performance and real-time constraints:

```python
import rclpy
from rclpy.node import Node
import tensorflow as tf
from sensor_msgs.msg import Image
from humanoid_robot_msgs.msg import VisionResult
import numpy as np
from cv_bridge import CvBridge
import time

class DeepVisionNode(Node):
    def __init__(self):
        super().__init__('deep_vision_node')
        self.bridge = CvBridge()

        # Load pre-trained model
        self.model = self.load_model()

        # Create subscription and publisher
        self.image_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.image_callback, 10)
        self.result_pub = self.create_publisher(VisionResult, '/vision_result', 10)

        # Performance monitoring
        self.inference_times = []
        self.frame_count = 0

        self.get_logger().info('Deep vision node initialized')

    def load_model(self):
        try:
            # Load a pre-trained model (e.g., MobileNetV2 for efficiency)
            # For humanoid robotics, lightweight models are preferred for real-time performance
            model = tf.keras.applications.MobileNetV2(
                input_shape=(224, 224, 3),
                include_top=True,
                weights='imagenet'
            )
            self.get_logger().info('Model loaded successfully')
            return model
        except Exception as e:
            self.get_logger().error(f'Failed to load model: {e}')
            return None

    def image_callback(self, msg):
        if self.model is None:
            return

        try:
            # Convert ROS image to OpenCV format
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Preprocess image for model
            input_image = self.preprocess_image(cv_image)

            # Run inference
            start_time = time.time()
            predictions = self.model.predict(input_image)
            end_time = time.time()

            # Calculate inference time
            inference_time = end_time - start_time
            self.inference_times.append(inference_time)
            self.frame_count += 1

            # Log performance metrics periodically
            if self.frame_count % 10 == 0:
                avg_time = np.mean(self.inference_times[-10:])
                self.get_logger().info(f'Average inference time: {avg_time:.3f}s')

            # Process results
            result = self.process_predictions(predictions)

            # Publish results
            result_msg = VisionResult()
            result_msg.objects = result['objects']
            result_msg.confidence = result['confidence']
            result_msg.processing_time = inference_time
            result_msg.header.stamp = self.get_clock().now().to_msg()
            self.result_pub.publish(result_msg)

        except Exception as e:
            self.get_logger().error(f'Error in image callback: {e}')

    def preprocess_image(self, image):
        # Resize and normalize image
        resized = cv2.resize(image, (224, 224))
        normalized = resized / 255.0
        batched = np.expand_dims(normalized, axis=0)
        return batched

    def process_predictions(self, predictions):
        # Convert model predictions to meaningful results
        # Get top 3 predictions
        top_indices = np.argsort(predictions[0])[::-1][:3]
        top_predictions = []
        top_confidences = []

        # Load ImageNet labels (simplified)
        imagenet_labels = tf.keras.applications.mobilenet_v2.decode_predictions(
            predictions, top=3
        )[0]

        for _, class_name, confidence in imagenet_labels:
            top_predictions.append(class_name)
            top_confidences.append(float(confidence))

        return {
            'objects': top_predictions,
            'confidence': top_confidences
        }

def main(args=None):
    rclpy.init(args=args)
    vision_node = DeepVisionNode()

    try:
        rclpy.spin(vision_node)
    except KeyboardInterrupt:
        vision_node.get_logger().info('Interrupted by user')
    finally:
        vision_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Natural Language Processing with rclpy

### Speech Recognition and Understanding

Integrating NLP capabilities enables human-robot interaction using rclpy:

```python
import rclpy
from rclpy.node import Node
import speech_recognition as sr
from std_msgs.msg import String
from humanoid_robot_msgs.msg import RobotCommand
import nltk
from transformers import pipeline
import threading

class NLPUnderstandingNode(Node):
    def __init__(self):
        super().__init__('nlp_understanding_node')

        # Initialize speech recognition
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()

        # Set up microphone for ambient noise adjustment
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source)

        # Initialize NLP pipeline for command understanding
        try:
            # Use a pre-trained model for text classification
            self.nlp_pipeline = pipeline(
                "text-classification",
                model="microsoft/DialoGPT-medium"
            )
            self.nlp_available = True
        except Exception as e:
            self.get_logger().warn(f'NLP pipeline not available: {e}')
            self.nlp_available = False

        # Publishers and subscribers
        self.speech_pub = self.create_publisher(String, '/speech_text', 10)
        self.command_pub = self.create_publisher(RobotCommand, '/robot_command', 10)

        # Start listening in a separate thread
        self.listening_active = True
        self.listening_thread = threading.Thread(target=self.listen_for_speech, daemon=True)
        self.listening_thread.start()

        self.get_logger().info('NLP understanding node initialized')

    def listen_for_speech(self):
        while self.listening_active:
            try:
                with self.microphone as source:
                    self.get_logger().debug('Listening...')
                    audio = self.recognizer.listen(source, timeout=5.0, phrase_time_limit=5.0)

                # Convert speech to text
                text = self.recognizer.recognize_google(audio)
                self.get_logger().info(f'Recognized: {text}')

                # Publish recognized text
                text_msg = String()
                text_msg.data = text
                self.speech_pub.publish(text_msg)

                # Process command using NLP
                self.process_command(text)

            except sr.WaitTimeoutError:
                # No speech detected, continue listening
                continue
            except sr.UnknownValueError:
                self.get_logger().info('Could not understand speech')
            except sr.RequestError as e:
                self.get_logger().error(f'Speech recognition error: {e}')
            except Exception as e:
                self.get_logger().error(f'Unexpected error: {e}')

    def process_command(self, text):
        # Simple command processing - in practice, use more sophisticated NLP
        command = self.classify_command(text)

        # Create and publish robot command
        cmd_msg = RobotCommand()
        cmd_msg.command = command['type']
        cmd_msg.parameters = str(command['params'])
        cmd_msg.confidence = command['confidence']
        cmd_msg.timestamp = self.get_clock().now().to_msg()

        self.command_pub.publish(cmd_msg)

        self.get_logger().info(f'Processed command: {command}')

    def classify_command(self, text):
        # Simple keyword-based command classification
        text_lower = text.lower()

        if any(word in text_lower for word in ['hello', 'hi', 'greet', 'hey']):
            return {'type': 'GREET', 'params': {}, 'confidence': 0.9}
        elif any(word in text_lower for word in ['move', 'go', 'walk', 'step']):
            direction = 'forward'  # Could extract direction from text
            return {'type': 'MOVE', 'params': {'direction': direction}, 'confidence': 0.8}
        elif any(word in text_lower for word in ['dance', 'move', 'groove']):
            return {'type': 'DANCE', 'params': {}, 'confidence': 0.85}
        elif any(word in text_lower for word in ['stop', 'halt', 'pause']):
            return {'type': 'STOP', 'params': {}, 'confidence': 0.95}
        else:
            return {'type': 'UNKNOWN', 'params': {'text': text}, 'confidence': 0.3}

    def destroy_node(self):
        self.listening_active = False
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    nlp_node = NLPUnderstandingNode()

    try:
        rclpy.spin(nlp_node)
    except KeyboardInterrupt:
        nlp_node.get_logger().info('Interrupted by user')
    finally:
        nlp_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Reinforcement Learning with rclpy

### Environment Integration

Reinforcement learning requires a well-defined environment for training using rclpy:

```python
import rclpy
from rclpy.node import Node
from humanoid_robot_msgs.msg import RobotState, Action, Reward
import numpy as np
from collections import deque
import random

class RLEnvironmentNode(Node):
    def __init__(self):
        super().__init__('rl_environment_node')

        # Publishers and subscribers for RL communication
        self.state_pub = self.create_publisher(RobotState, '/rl_state', 10)
        self.action_sub = self.create_subscription(
            Action, '/rl_action', self.action_callback, 10)
        self.reward_pub = self.create_publisher(Reward, '/rl_reward', 10)

        # Initialize RL environment parameters
        self.current_state = np.zeros(24)  # Example state vector
        self.episode_count = 0
        self.step_count = 0
        self.max_steps_per_episode = 1000

        # For training, we might also have:
        self.replay_buffer = deque(maxlen=10000)
        self.epsilon = 1.0  # For epsilon-greedy exploration
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

        self.get_logger().info('RL environment node initialized')

    def action_callback(self, msg):
        # Convert ROS action message to numpy array
        action = np.array(msg.data)

        # Execute action in environment (simulation or real robot)
        new_state, reward, done = self.execute_action(action)

        # Store experience for training
        experience = (self.current_state, action, reward, new_state, done)
        self.replay_buffer.append(experience)

        # Update current state
        self.current_state = new_state
        self.step_count += 1

        # Check if episode should end
        if done or self.step_count >= self.max_steps_per_episode:
            self.episode_count += 1
            self.step_count = 0
            self.current_state = self.reset_environment()

        # Publish new state and reward
        state_msg = RobotState()
        state_msg.state = self.current_state.tolist()
        state_msg.header.stamp = self.get_clock().now().to_msg()
        self.state_pub.publish(state_msg)

        reward_msg = Reward()
        reward_msg.value = reward
        reward_msg.done = done
        reward_msg.episode = self.episode_count
        reward_msg.header.stamp = self.get_clock().now().to_msg()
        self.reward_pub.publish(reward_msg)

    def execute_action(self, action):
        # This would interface with the actual robot or simulation
        # For this example, we'll simulate a simple environment
        # In practice, this would call into Gazebo simulation or real robot

        # Example: simple cart-pole-like environment simulation
        # Update state based on action
        new_state = self.current_state + action * 0.1  # Simplified dynamics
        new_state = np.clip(new_state, -1.0, 1.0)  # Keep state within bounds

        # Calculate reward based on state
        # Reward for staying upright and centered
        reward = 1.0 - np.mean(np.abs(new_state))  # Higher reward for being centered

        # Check if episode is done (e.g., robot fell over)
        done = np.any(np.abs(new_state) > 0.95)  # Episode ends if state is too extreme

        return new_state, reward, done

    def reset_environment(self):
        # Reset environment to initial state
        self.current_state = np.random.uniform(-0.1, 0.1, size=(24,))
        self.get_logger().info(f'Reset environment, episode: {self.episode_count}')
        return self.current_state

def main(args=None):
    rclpy.init(args=args)
    rl_node = RLEnvironmentNode()

    try:
        rclpy.spin(rl_node)
    except KeyboardInterrupt:
        rl_node.get_logger().info('Interrupted by user')
    finally:
        rl_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Performance Optimization Strategies with rclpy

### Efficient Message Handling

Optimizing AI-ROS integration for performance using rclpy:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
import threading
from queue import Queue, Empty
import time

class OptimizedAINode(Node):
    def __init__(self):
        super().__init__('optimized_ai_node')
        self.bridge = CvBridge()

        # Message queue to handle bursts of data
        self.image_queue = Queue(maxsize=10)  # Limit queue size to prevent memory issues

        # Subscribe to image topic
        self.image_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.image_callback, 1)

        # Publisher for results
        self.result_pub = self.create_publisher(String, '/ai_result', 10)

        # Processing thread
        self.processing_thread = threading.Thread(target=self.process_images, daemon=True)
        self.processing_thread.start()

        # Performance metrics
        self.processed_count = 0
        self.start_time = time.time()

        self.get_logger().info('Optimized AI node initialized')

    def image_callback(self, msg):
        # Non-blocking queue put to avoid blocking the ROS callback
        try:
            self.image_queue.put_nowait(msg)
        except:
            # Queue is full, drop the message
            self.get_logger().warn('Image queue full, dropping frame')

    def process_images(self):
        # Processing loop in separate thread
        while rclpy.ok():
            try:
                # Get image from queue with timeout
                msg = self.image_queue.get(timeout=0.1)

                # Process the image
                result = self.ai_process_image(msg)

                # Publish result
                result_msg = String()
                result_msg.data = result
                self.result_pub.publish(result_msg)

                self.processed_count += 1

                # Log performance periodically
                if self.processed_count % 50 == 0:
                    elapsed = time.time() - self.start_time
                    fps = self.processed_count / elapsed
                    self.get_logger().info(f'Processed {self.processed_count} images, FPS: {fps:.2f}')

            except Empty:
                # No images to process, continue
                continue
            except Exception as e:
                self.get_logger().error(f'Error in processing thread: {e}')

    def ai_process_image(self, msg):
        # Convert ROS image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Simple AI processing (replace with actual AI model)
        # For example, detect if image is mostly light or dark
        avg_brightness = cv2.mean(cv_image)[0]
        result = "bright" if avg_brightness > 128 else "dark"

        return f"Image is {result}, avg brightness: {avg_brightness:.2f}"

def main(args=None):
    rclpy.init(args=args)
    ai_node = OptimizedAINode()

    try:
        rclpy.spin(ai_node)
    except KeyboardInterrupt:
        ai_node.get_logger().info('Interrupted by user')
    finally:
        ai_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Best Practices for AI-ROS Integration with rclpy

### 1. Real-time Performance
- Use appropriate QoS settings for AI data streams
- Implement asynchronous processing where possible
- Monitor and optimize inference times
- Use threading for CPU-intensive operations

### 2. Safety and Reliability
- Implement fallback behaviors when AI fails
- Validate AI outputs before execution
- Use redundancy for critical AI functions
- Implement graceful degradation

### 3. Scalability
- Design modular AI components
- Use appropriate communication patterns
- Consider distributed AI processing
- Implement proper error handling

### 4. Debugging and Monitoring
- Log AI model inputs and outputs
- Monitor model performance metrics
- Implement visualization tools for AI decisions
- Use ROS 2 tools like rqt for monitoring

## Hands-on Exercise

1. Create a simple AI node using rclpy that performs basic image classification
2. Integrate the AI node with ROS 2 message passing
3. Create a subscriber that processes the AI results
4. Implement a basic NLP module for simple command recognition
5. Test the integration and measure performance metrics

## Quiz Questions

1. What are the main advantages of using rclpy for AI-ROS integration?
2. How can you optimize deep learning models for real-time robotics applications in Python?
3. What safety measures should be implemented when using AI for robot control with rclpy?

</InteractiveLesson>