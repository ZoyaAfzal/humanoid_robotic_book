"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[935],{5959:(n,e,t)=>{t.d(e,{A:()=>p});var s=t(6540),i=t(4848);const a=({totalLessons:n,completedLessons:e,currentLesson:t="Current Lesson"})=>{const[a,o]=(0,s.useState)(0);return(0,s.useEffect)(()=>{const t=setTimeout(()=>{o(e/n*100)},300);return()=>clearTimeout(t)},[e,n]),(0,i.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,i.jsxs)("div",{className:"row",children:[(0,i.jsxs)("div",{className:"col col--8",children:[(0,i.jsx)("h3",{children:"Learning Progress"}),(0,i.jsxs)("p",{children:["Currently studying: ",(0,i.jsx)("strong",{children:t})]})]}),(0,i.jsx)("div",{className:"col col--4 text--right",children:(0,i.jsxs)("span",{className:"badge badge--primary",children:[Math.round(a),"% Complete"]})})]}),(0,i.jsxs)("div",{className:"margin-top--md",children:[(0,i.jsx)("div",{className:"progress-indicator",children:(0,i.jsx)("div",{className:"progress-bar",style:{width:`${a}%`,transition:"width 1s ease-in-out"}})}),(0,i.jsx)("div",{className:"margin-top--sm",children:(0,i.jsxs)("small",{children:[e," of ",n," lessons completed"]})})]}),(0,i.jsxs)("div",{className:"margin-top--md",children:[(0,i.jsx)("button",{className:"button button--primary button--sm margin-right--sm",onClick:()=>alert("Lesson marked as completed!"),children:"\u2713 Mark Complete"}),(0,i.jsx)("button",{className:"button button--secondary button--sm",onClick:()=>alert("Lesson bookmarked!"),children:"\u2605 Bookmark"})]})]})},o=({title:n,questions:e})=>{const[t,a]=(0,s.useState)(Array(e.length).fill(null)),[o,r]=(0,s.useState)(!1),[l,c]=(0,s.useState)(!1),p=e.reduce((n,e,s)=>t[s]===e.correctAnswer?n+1:n,0);return(0,i.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,i.jsx)("h3",{children:n}),e.map((n,e)=>(0,i.jsxs)("div",{className:"margin-bottom--md",children:[(0,i.jsxs)("h4",{children:["Question ",e+1,": ",n.question]}),(0,i.jsx)("div",{className:"margin-left--md",children:n.options.map((n,s)=>(0,i.jsx)("div",{className:"margin-bottom--sm",children:(0,i.jsxs)("label",{style:{display:"flex",alignItems:"center"},children:[(0,i.jsx)("input",{type:"radio",name:`question-${e}`,checked:t[e]===s,onChange:()=>((n,e)=>{if(l)return;const s=[...t];s[n]=e,a(s)})(e,s),disabled:l,style:{marginRight:"0.5rem"}}),n]})},s))}),o&&(0,i.jsx)("div",{className:"margin-top--sm padding--sm "+(t[e]===n.correctAnswer?"alert alert--success":"alert alert--danger"),children:t[e]===n.correctAnswer?(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"\u2713 Correct!"})}):null!==t[e]?(0,i.jsxs)("p",{children:[(0,i.jsx)("strong",{children:"\u2717 Incorrect."})," Correct answer: ",n.options[n.correctAnswer],(0,i.jsx)("br",{}),(0,i.jsx)("small",{children:n.explanation})]}):(0,i.jsx)("p",{children:(0,i.jsx)("small",{children:n.explanation})})})]},n.id)),l?(0,i.jsxs)("div",{children:[(0,i.jsxs)("div",{className:"alert alert--success margin-bottom--md",children:[(0,i.jsxs)("h4",{children:["Quiz Results: ",p,"/",e.length," correct"]}),(0,i.jsxs)("p",{children:["You scored ",Math.round(p/e.length*100),"%!"]})]}),(0,i.jsx)("button",{className:"button button--secondary",onClick:()=>{a(Array(e.length).fill(null)),r(!1),c(!1)},children:"Try Again"})]}):(0,i.jsx)("button",{className:"button button--primary",onClick:()=>{c(!0),r(!0)},children:"Submit Quiz"})]})},r=[{id:1,question:"What does ROS stand for?",options:["Robot Operating System","Robotics Operating Software","Remote Operating System","Robotic Operation Suite"],correctAnswer:0,explanation:"ROS stands for Robot Operating System, a flexible framework for writing robot software."},{id:2,question:"Which of the following is a core concept in ROS 2?",options:["Nodes and Topics","Classes and Objects","Functions and Variables","Databases and Tables"],correctAnswer:0,explanation:"Nodes and Topics are fundamental concepts in ROS 2 for communication between different parts of a robot system."}],l=()=>(0,i.jsx)(o,{title:"ROS 2 Fundamentals Quiz",questions:r});var c=t(8774);const p=({children:n,title:e="Lesson",chapter:t=1,lesson:s=1})=>{const o=s>1?s-1:null,r=s<3?s+1:null,p=`/docs/chapter${t}`,d=(n,e)=>{if(1===n){if(1===e)return"spec-kit-plus-workflow";if(2===e)return"physical-ai-embodied-intelligence";if(3===e)return"development-environment-setup"}else if(2===n){if(1===e)return"ros2-architecture";if(2===e)return"humanoid-robot-modeling";if(3===e)return"bridging-ai-agents"}else if(3===n){if(1===e)return"gazebo-environment-setup";if(2===e)return"simulating-physics-collisions";if(3===e)return"sensor-simulation"}else if(4===n){if(1===e)return"isaac-sim-synthetic-data";if(2===e)return"hardware-accelerated-navigation";if(3===e)return"bipedal-path-planning"}else if(5===n){if(1===e)return"voice-to-action";if(2===e)return"cognitive-planning";if(3===e)return"capstone-project-execution"}return"spec-kit-plus-workflow"};d(t,s);return(0,i.jsxs)("div",{className:"interactive-lesson",children:[(0,i.jsxs)("div",{className:"chapter-header margin-bottom--lg",children:[(0,i.jsxs)("span",{className:"chapter-indicator",children:["Chapter ",t," \u2022 Lesson ",s]}),(0,i.jsx)("h1",{children:e})]}),(0,i.jsx)(a,{totalLessons:3,completedLessons:s-1,currentLesson:e}),(0,i.jsx)("div",{className:"lesson-content",children:n}),(0,i.jsxs)("div",{className:"margin-top--xl",children:[(0,i.jsx)("h3",{children:"Knowledge Check"}),(0,i.jsx)(l,{})]}),(0,i.jsx)("div",{className:"margin-top--lg",children:(0,i.jsxs)("div",{className:"row",children:[(0,i.jsx)("div",{className:"col col--6",children:o&&(0,i.jsx)(c.A,{to:`${p}/lesson${o}/${d(t,o)}`,className:"button button--secondary",children:"\u2190 Previous Lesson"})}),(0,i.jsx)("div",{className:"col col--6 text--right",children:r&&(0,i.jsx)(c.A,{to:`${p}/lesson${r}/${d(t,r)}`,className:"button button--primary",children:"Next Lesson \u2192"})})]})}),(0,i.jsxs)("div",{className:"margin-top--lg interactive-controls-container",children:[(0,i.jsx)("button",{className:"personalize-button",onClick:()=>alert("Content personalized!"),children:"\ud83c\udfaf Personalize Learning"}),(0,i.jsx)("button",{className:"translate-button",onClick:()=>alert("Content translated to Urdu!"),children:"\ud83c\udf10 Translate to Urdu"})]})]})}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),s.createElement(a.Provider,{value:e},n.children)}},9760:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"chapter5/lesson2/cognitive-planning","title":"cognitive-planning","description":"In this comprehensive lesson, you\'ll explore cognitive planning systems that leverage Large Language Models (LLMs) to generate complex action sequences for humanoid robots. Cognitive planning bridges high-level human instructions with low-level robot actions through intelligent reasoning and planning capabilities.","source":"@site/docs/chapter5/lesson2/cognitive-planning.mdx","sourceDirName":"chapter5/lesson2","slug":"/chapter5/lesson2/cognitive-planning","permalink":"/humanoid_robotic_book/docs/chapter5/lesson2/cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"mySidebar","previous":{"title":"voice-to-action","permalink":"/humanoid_robotic_book/docs/chapter5/lesson1/voice-to-action"},"next":{"title":"capstone-project-execution","permalink":"/humanoid_robotic_book/docs/chapter5/lesson3/capstone-project-execution"}}');var i=t(4848),a=t(8453),o=t(5959);const r={sidebar_position:1},l="Cognitive Planning (LLM to ROS Action Sequence)",c={},p=[{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"Key Components of Cognitive Planning Systems",id:"key-components-of-cognitive-planning-systems",level:3},{value:"Large Language Model Integration",id:"large-language-model-integration",level:2},{value:"LLM Selection and Configuration",id:"llm-selection-and-configuration",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"World State Management",id:"world-state-management",level:3},{value:"Advanced Planning Algorithms",id:"advanced-planning-algorithms",level:2},{value:"Hierarchical Task Network (HTN) Planning",id:"hierarchical-task-network-htn-planning",level:3},{value:"LLM-Enhanced Planning Pipeline",id:"llm-enhanced-planning-pipeline",level:2},{value:"Complete Cognitive Planning System",id:"complete-cognitive-planning-system",level:3},{value:"Context Learning and Adaptation",id:"context-learning-and-adaptation",level:2},{value:"Learning from Execution Feedback",id:"learning-from-execution-feedback",level:3},{value:"Planning Validation and Safety",id:"planning-validation-and-safety",level:2},{value:"Safety-Constrained Planning",id:"safety-constrained-planning",level:3},{value:"Best Practices for Cognitive Planning",id:"best-practices-for-cognitive-planning",level:2},{value:"1. Robustness and Error Handling",id:"1-robustness-and-error-handling",level:3},{value:"2. Performance Optimization",id:"2-performance-optimization",level:3},{value:"3. Safety and Reliability",id:"3-safety-and-reliability",level:3},{value:"4. Learning and Adaptation",id:"4-learning-and-adaptation",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. LLM Hallucination",id:"1-llm-hallucination",level:3},{value:"2. Planning Inconsistency",id:"2-planning-inconsistency",level:3},{value:"3. Computational Performance",id:"3-computational-performance",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Quiz Questions",id:"quiz-questions",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(o.A,{title:"Cognitive Planning (LLM to ROS Action Sequence)",chapter:5,lesson:2,children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-llm-to-ros-action-sequence",children:"Cognitive Planning (LLM to ROS Action Sequence)"})}),(0,i.jsx)(e.p,{children:"In this comprehensive lesson, you'll explore cognitive planning systems that leverage Large Language Models (LLMs) to generate complex action sequences for humanoid robots. Cognitive planning bridges high-level human instructions with low-level robot actions through intelligent reasoning and planning capabilities."}),(0,i.jsx)(e.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),(0,i.jsx)(e.p,{children:"Cognitive planning for humanoid robots involves:"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"High-level Reasoning"}),": Understanding complex, abstract commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking down complex tasks into executable actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Awareness"}),": Adapting plans based on environment and state"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Learning and Adaptation"}),": Improving planning based on experience"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-modal Integration"}),": Combining language, perception, and action"]}),"\n"]}),(0,i.jsx)(e.h3,{id:"key-components-of-cognitive-planning-systems",children:"Key Components of Cognitive Planning Systems"}),(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Language Understanding"}),": Interpreting natural language commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"World Modeling"}),": Maintaining representation of environment and robot state"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Plan Generation"}),": Creating sequences of executable actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Plan Execution"}),": Coordinating with robot control systems"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feedback Integration"}),": Learning from execution outcomes"]}),"\n"]}),(0,i.jsx)(e.h2,{id:"large-language-model-integration",children:"Large Language Model Integration"}),(0,i.jsx)(e.h3,{id:"llm-selection-and-configuration",children:"LLM Selection and Configuration"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_robot_msgs.msg import TaskPlan\nimport openai\nimport json\nimport asyncio\nfrom typing import Dict, List, Any\n\nclass LLMPlanner(Node):\n    def __init__(self):\n        super().__init__(\'llm_planner\')\n\n        # Initialize LLM client\n        self.setup_llm_client()\n\n        # Publishers and subscribers\n        self.task_sub = self.create_subscription(\n            String, \'/high_level_task\', self.task_callback, 10)\n        self.plan_pub = self.create_publisher(\n            TaskPlan, \'/generated_plan\', 10)\n\n        # Robot state and environment context\n        self.robot_state = {}\n        self.environment_context = {}\n\n        self.get_logger().info(\'LLM-based cognitive planner initialized\')\n\n    def setup_llm_client(self):\n        """Setup LLM client for planning"""\n        # Using OpenAI API as an example\n        # In practice, you might use local models like Llama, Mistral, etc.\n        self.client = openai.OpenAI(\n            api_key=self.get_parameter_or(\'openai_api_key\', \'your-api-key\')\n        )\n\n        # Model configuration\n        self.model_name = "gpt-4-turbo"  # Or other suitable model\n        self.temperature = 0.1  # Lower temperature for more consistent planning\n\n    def task_callback(self, msg):\n        """Process high-level task request"""\n        task_description = msg.data\n        self.get_logger().info(f\'Received task: {task_description}\')\n\n        # Generate plan using LLM\n        plan = self.generate_plan(task_description)\n\n        if plan:\n            # Publish the generated plan\n            plan_msg = TaskPlan()\n            plan_msg.plan = json.dumps(plan)\n            plan_msg.timestamp = self.get_clock().now().to_msg()\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f\'Generated plan with {len(plan)} steps\')\n\n    def generate_plan(self, task_description: str) -> List[Dict[str, Any]]:\n        """Generate action plan using LLM"""\n        try:\n            # Create a detailed prompt for the LLM\n            prompt = self.create_planning_prompt(task_description)\n\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[\n                    {"role": "system", "content": self.get_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=self.temperature,\n                response_format={"type": "json_object"}  # Expect JSON response\n            )\n\n            # Parse the response\n            plan_json = json.loads(response.choices[0].message.content)\n            return plan_json.get(\'action_sequence\', [])\n\n        except Exception as e:\n            self.get_logger().error(f\'Error generating plan: {e}\')\n            return []\n\n    def create_planning_prompt(self, task_description: str) -> str:\n        """Create detailed prompt for planning"""\n        return f"""\n        Task: {task_description}\n\n        Current Robot State: {json.dumps(self.robot_state)}\n        Environment Context: {json.dumps(self.environment_context)}\n\n        Available Actions:\n        - navigate_to(location)\n        - pick_up(object)\n        - place_down(object, location)\n        - open_door(door_name)\n        - close_door(door_name)\n        - speak(text)\n        - wait(duration_seconds)\n        - detect_object(object_type)\n        - follow_person(person_name)\n        - avoid_obstacle(obstacle)\n\n        Generate a detailed action sequence to complete the task.\n        Return as JSON with the following structure:\n        {{\n            "action_sequence": [\n                {{\n                    "action": "action_name",\n                    "parameters": {{"param1": "value1", "param2": "value2"}},\n                    "description": "Brief description of what this action does",\n                    "expected_outcome": "What should happen after this action"\n                }}\n            ],\n            "reasoning": "Brief explanation of the planning logic"\n        }}\n\n        Be specific with locations and objects. Consider robot capabilities and environment constraints.\n        """\n\n    def get_system_prompt(self) -> str:\n        """Get system prompt for consistent behavior"""\n        return """\n        You are an expert robot task planner. Your job is to break down high-level human commands\n        into detailed, executable action sequences for a humanoid robot.\n\n        Guidelines:\n        1. Be specific about locations, objects, and parameters\n        2. Consider the robot\'s current state and environment\n        3. Generate safe, executable actions\n        4. Include error handling considerations\n        5. Return structured JSON response\n        6. Ensure each action is feasible with the robot\'s capabilities\n        """\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = LLMPlanner()\n\n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        planner.get_logger().info(\'Shutting down LLM planner\')\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),(0,i.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),(0,i.jsx)(e.h3,{id:"world-state-management",children:"World State Management"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import OccupancyGrid\nimport json\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass ObjectInstance:\n    name: str\n    type: str\n    pose: PoseStamped\n    confidence: float\n    properties: Dict[str, str]\n\n@dataclass\nclass Location:\n    name: str\n    pose: PoseStamped\n    type: str  # room, door, furniture, etc.\n    properties: Dict[str, str]\n\n@dataclass\nclass RobotState:\n    pose: PoseStamped\n    battery_level: float\n    current_task: str\n    available_actions: List[str]\n    equipment_status: Dict[str, bool]\n\nclass WorldStateManager(Node):\n    def __init__(self):\n        super().__init__('world_state_manager')\n\n        # Initialize world state\n        self.robot_state = RobotState(\n            pose=PoseStamped(),\n            battery_level=1.0,\n            current_task=\"\",\n            available_actions=[],\n            equipment_status={}\n        )\n        self.objects = {}\n        self.locations = {}\n        self.environment_map = None\n\n        # Subscriptions for state updates\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/robot_pose', self.pose_callback, 10)\n        self.object_sub = self.create_subscription(\n            String, '/detected_objects', self.object_callback, 10)\n        self.map_sub = self.create_subscription(\n            OccupancyGrid, '/map', self.map_callback, 10)\n        self.battery_sub = self.create_subscription(\n            String, '/battery_status', self.battery_callback, 10)\n\n        # Timer for state updates\n        self.state_update_timer = self.create_timer(1.0, self.update_context)\n\n    def pose_callback(self, msg):\n        \"\"\"Update robot pose\"\"\"\n        self.robot_state.pose = msg\n\n    def object_callback(self, msg):\n        \"\"\"Update detected objects\"\"\"\n        try:\n            objects_data = json.loads(msg.data)\n            for obj_data in objects_data:\n                obj = ObjectInstance(\n                    name=obj_data['name'],\n                    type=obj_data['type'],\n                    pose=obj_data['pose'],\n                    confidence=obj_data['confidence'],\n                    properties=obj_data.get('properties', {})\n                )\n                self.objects[obj.name] = obj\n        except Exception as e:\n            self.get_logger().error(f'Error parsing object data: {e}')\n\n    def map_callback(self, msg):\n        \"\"\"Update environment map\"\"\"\n        self.environment_map = msg\n\n    def battery_callback(self, msg):\n        \"\"\"Update battery status\"\"\"\n        try:\n            battery_data = json.loads(msg.data)\n            self.robot_state.battery_level = battery_data['level']\n        except Exception as e:\n            self.get_logger().error(f'Error parsing battery data: {e}')\n\n    def update_context(self):\n        \"\"\"Update planning context with current state\"\"\"\n        # This method is called periodically to ensure context is current\n        # for planning decisions\n        pass\n\n    def get_context_for_planning(self) -> Dict[str, Any]:\n        \"\"\"Get current context for planning\"\"\"\n        return {\n            'robot_state': {\n                'position': {\n                    'x': self.robot_state.pose.pose.position.x,\n                    'y': self.robot_state.pose.pose.position.y,\n                    'z': self.robot_state.pose.pose.position.z\n                },\n                'battery_level': self.robot_state.battery_level,\n                'available_actions': self.robot_state.available_actions\n            },\n            'objects': [\n                {\n                    'name': obj.name,\n                    'type': obj.type,\n                    'position': {\n                        'x': obj.pose.pose.position.x,\n                        'y': obj.pose.pose.position.y,\n                        'z': obj.pose.pose.position.z\n                    },\n                    'confidence': obj.confidence\n                } for obj in self.objects.values()\n            ],\n            'locations': [\n                {\n                    'name': loc.name,\n                    'type': loc.type,\n                    'position': {\n                        'x': loc.pose.pose.position.x,\n                        'y': loc.pose.pose.position.y,\n                        'z': loc.pose.pose.position.z\n                    }\n                } for loc in self.locations.values()\n            ],\n            'environment_map_available': self.environment_map is not None\n        }\n\n    def get_relevant_objects(self, object_type: str) -> List[ObjectInstance]:\n        \"\"\"Get objects of specific type near robot\"\"\"\n        relevant_objects = []\n        robot_pos = self.robot_state.pose.pose.position\n\n        for obj in self.objects.values():\n            if obj.type == object_type:\n                # Calculate distance to robot\n                dist = ((obj.pose.pose.position.x - robot_pos.x) ** 2 +\n                       (obj.pose.pose.position.y - robot_pos.y) ** 2) ** 0.5\n\n                if dist < 5.0:  # Within 5 meters\n                    relevant_objects.append(obj)\n\n        return sorted(relevant_objects, key=lambda x:\n                     ((x.pose.pose.position.x - robot_pos.x) ** 2 +\n                      (x.pose.pose.position.y - robot_pos.y) ** 2) ** 0.5)\n"})}),(0,i.jsx)(e.h2,{id:"advanced-planning-algorithms",children:"Advanced Planning Algorithms"}),(0,i.jsx)(e.h3,{id:"hierarchical-task-network-htn-planning",children:"Hierarchical Task Network (HTN) Planning"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom typing import List, Dict, Any, Callable\nimport json\n\n@dataclass\nclass Task:\n    name: str\n    parameters: Dict[str, Any]\n    preconditions: List[str]\n    effects: List[str]\n\n@dataclass\nclass Method:\n    name: str\n    task: str\n    subtasks: List[\'Task\']\n    conditions: List[str]\n\nclass HTNPlanner:\n    def __init__(self):\n        self.tasks = {}\n        self.methods = {}\n        self.state = set()\n\n        # Define basic methods\n        self.define_methods()\n\n    def define_methods(self):\n        """Define hierarchical planning methods"""\n        # Example: Deliver object method\n        deliver_method = Method(\n            name="deliver_object_method",\n            task="DELIVER_OBJECT",\n            subtasks=[\n                Task("NAVIGATE_TO", {"location": "source"}, [], ["at_location(source)"]),\n                Task("PICK_UP", {"object": "target_object"}, ["at_location(source)"], ["holding(target_object)"]),\n                Task("NAVIGATE_TO", {"location": "destination"}, ["holding(target_object)"], ["at_location(destination)"]),\n                Task("PLACE_DOWN", {"object": "target_object", "location": "destination"},\n                     ["at_location(destination)", "holding(target_object)"], ["placed(target_object)"])\n            ],\n            conditions=["object_available(target_object)", "location_accessible(destination)"]\n        )\n\n        self.methods["DELIVER_OBJECT"] = [deliver_method]\n\n        # Example: Clean room method\n        clean_room_method = Method(\n            name="clean_room_method",\n            task="CLEAN_ROOM",\n            subtasks=[\n                Task("NAVIGATE_TO", {"location": "room_entrance"}, [], ["at_location(room_entrance)"]),\n                Task("DETECT_DIRT"),  # Custom task to find dirty spots\n                Task("NAVIGATE_TO", {"location": "dirt_location"}, ["dirt_detected"], ["at_location(dirt_location)"]),\n                Task("CLEAN_AREA", {"location": "dirt_location"}, ["at_location(dirt_location)"], ["area_clean(dirt_location)"]),\n                Task("NAVIGATE_TO", {"location": "room_exit"}, [], ["at_location(room_exit)"])\n            ],\n            conditions=["room_accessible", "cleaning_equipment_available"]\n        )\n\n        self.methods["CLEAN_ROOM"] = [clean_room_method]\n\n    def plan(self, task: Task) -> List[Task]:\n        """Generate plan for high-level task"""\n        return self.decompose_task(task, self.state)\n\n    def decompose_task(self, task: Task, state: set) -> List[Task]:\n        """Decompose task into subtasks using HTN methods"""\n        # Check if task is primitive (can be executed directly)\n        if self.is_primitive_task(task):\n            return [task]\n\n        # Find applicable methods for this task\n        applicable_methods = self.get_applicable_methods(task, state)\n\n        for method in applicable_methods:\n            # Check if method conditions are satisfied\n            if self.check_conditions(method.conditions, state):\n                # Recursively decompose subtasks\n                plan = []\n                for subtask in method.subtasks:\n                    subplan = self.decompose_task(subtask, state)\n                    plan.extend(subplan)\n                    # Update state with effects of completed subtasks\n                    state.update(self.get_effects(subtask))\n\n                return plan\n\n        # If no method applies, return empty plan\n        return []\n\n    def is_primitive_task(self, task: Task) -> bool:\n        """Check if task is primitive (can be executed directly)"""\n        primitive_tasks = {\n            "NAVIGATE_TO", "PICK_UP", "PLACE_DOWN", "SPEAK",\n            "OPEN_DOOR", "CLOSE_DOOR", "WAIT", "DETECT_OBJECT"\n        }\n        return task.name in primitive_tasks\n\n    def get_applicable_methods(self, task: Task, state: set) -> List[Method]:\n        """Get methods applicable to the given task"""\n        return self.methods.get(task.name, [])\n\n    def check_conditions(self, conditions: List[str], state: set) -> bool:\n        """Check if all conditions are satisfied in current state"""\n        for condition in conditions:\n            if condition not in state:\n                return False\n        return True\n\n    def get_effects(self, task: Task) -> List[str]:\n        """Get effects of executing a task"""\n        return task.effects\n'})}),(0,i.jsx)(e.h2,{id:"llm-enhanced-planning-pipeline",children:"LLM-Enhanced Planning Pipeline"}),(0,i.jsx)(e.h3,{id:"complete-cognitive-planning-system",children:"Complete Cognitive Planning System"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_robot_msgs.msg import TaskPlan, ActionSequence\nfrom geometry_msgs.msg import PoseStamped\nimport json\nimport asyncio\nimport openai\nfrom typing import Dict, List, Any, Optional\n\nclass CognitivePlanningSystem(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planning_system\')\n\n        # Initialize components\n        self.world_state_manager = WorldStateManager(self)\n        self.hierarchical_planner = HTNPlanner()\n\n        # Setup LLM client\n        self.setup_llm_client()\n\n        # Publishers and subscribers\n        self.high_level_task_sub = self.create_subscription(\n            String, \'/high_level_task\', self.high_level_task_callback, 10)\n        self.action_plan_pub = self.create_publisher(\n            ActionSequence, \'/action_sequence\', 10)\n\n        # State tracking\n        self.current_plan = None\n        self.plan_execution_status = "idle"\n\n        self.get_logger().info(\'Cognitive planning system initialized\')\n\n    def setup_llm_client(self):\n        """Setup LLM client for cognitive planning"""\n        try:\n            self.client = openai.OpenAI(\n                api_key=self.get_parameter_or(\'openai_api_key\', \'your-api-key\')\n            )\n            self.model_name = "gpt-4-turbo"\n            self.get_logger().info(\'LLM client initialized successfully\')\n        except Exception as e:\n            self.get_logger().error(f\'Failed to initialize LLM client: {e}\')\n            self.client = None\n\n    def high_level_task_callback(self, msg):\n        """Process high-level cognitive task"""\n        task_description = msg.data\n        self.get_logger().info(f\'Received cognitive task: {task_description}\')\n\n        # Get current world context\n        context = self.world_state_manager.get_context_for_planning()\n\n        # Generate cognitive plan using LLM\n        cognitive_plan = self.generate_cognitive_plan(task_description, context)\n\n        if cognitive_plan:\n            # Convert to executable action sequence\n            action_sequence = self.convert_to_action_sequence(cognitive_plan)\n\n            # Publish the action sequence\n            action_msg = ActionSequence()\n            action_msg.sequence = json.dumps(action_sequence)\n            action_msg.task_description = task_description\n            action_msg.timestamp = self.get_clock().now().to_msg()\n            self.action_plan_pub.publish(action_msg)\n\n            self.current_plan = action_sequence\n            self.get_logger().info(f\'Published action sequence with {len(action_sequence)} actions\')\n\n    def generate_cognitive_plan(self, task_description: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:\n        """Generate cognitive plan using LLM"""\n        if not self.client:\n            self.get_logger().error(\'LLM client not available\')\n            return []\n\n        try:\n            # Create comprehensive prompt\n            prompt = self.create_cognitive_planning_prompt(task_description, context)\n\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[\n                    {"role": "system", "content": self.get_cognitive_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,\n                response_format={"type": "json_object"}\n            )\n\n            result = json.loads(response.choices[0].message.content)\n            return result.get(\'action_sequence\', [])\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in cognitive planning: {e}\')\n            return []\n\n    def create_cognitive_planning_prompt(self, task_description: str, context: Dict[str, Any]) -> str:\n        """Create prompt for cognitive planning"""\n        return f"""\n        Task: {task_description}\n\n        Robot Capabilities:\n        - Navigation: Can move to specified locations\n        - Manipulation: Can pick up and place objects (weight < 5kg)\n        - Perception: Can detect objects, people, obstacles\n        - Communication: Can speak and listen to voice commands\n        - Interaction: Can open/close doors, operate switches\n\n        Current Context:\n        {json.dumps(context, indent=2)}\n\n        Available Actions:\n        - navigate_to(location_name, x, y, z)\n        - pick_up(object_name)\n        - place_down(object_name, location_name)\n        - speak(text_message)\n        - detect_object(object_type)\n        - open_door(door_name)\n        - close_door(door_name)\n        - wait(seconds)\n        - follow_person(person_name)\n        - ask_for_help()\n\n        Generate a detailed cognitive action plan that:\n        1. Breaks down the high-level task into concrete steps\n        2. Considers the current robot state and environment\n        3. Handles potential failures and contingencies\n        4. Ensures safety and efficiency\n\n        Return as JSON:\n        {{\n            "action_sequence": [\n                {{\n                    "action": "action_name",\n                    "parameters": {{"param1": "value1"}},\n                    "description": "What this action does",\n                    "reasoning": "Why this action is needed",\n                    "expected_outcome": "What should happen",\n                    "failure_recovery": "What to do if action fails"\n                }}\n            ],\n            "overall_strategy": "High-level approach",\n            "safety_considerations": ["list", "of", "safety", "factors"],\n            "success_criteria": "How to know task is complete"\n        }}\n        """\n\n    def get_cognitive_system_prompt(self) -> str:\n        """System prompt for cognitive planning"""\n        return """\n        You are an expert cognitive planning system for humanoid robots.\n        Your role is to decompose high-level human commands into detailed,\n        executable action sequences considering the robot\'s capabilities,\n        current state, and environment.\n\n        Requirements:\n        1. Generate safe, executable actions\n        2. Include error handling and recovery strategies\n        3. Consider robot limitations and environment constraints\n        4. Provide clear reasoning for each action\n        5. Ensure the plan is complete and coherent\n        """\n\n    def convert_to_action_sequence(self, cognitive_plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        """Convert cognitive plan to executable action sequence"""\n        action_sequence = []\n\n        for plan_step in cognitive_plan:\n            action = {\n                \'action\': plan_step.get(\'action\', \'\'),\n                \'parameters\': plan_step.get(\'parameters\', {}),\n                \'description\': plan_step.get(\'description\', \'\'),\n                \'reasoning\': plan_step.get(\'reasoning\', \'\'),\n                \'expected_outcome\': plan_step.get(\'expected_outcome\', \'\'),\n                \'failure_recovery\': plan_step.get(\'failure_recovery\', \'\'),\n                \'priority\': 1,  # Default priority\n                \'timeout\': 30.0  # Default timeout in seconds\n            }\n            action_sequence.append(action)\n\n        return action_sequence\n\n    def execute_plan_step(self, step: Dict[str, Any]) -> bool:\n        """Execute a single plan step"""\n        # This would interface with the robot\'s action execution system\n        # For now, we\'ll just log the action\n        self.get_logger().info(f\'Executing action: {step["action"]} with params: {step["parameters"]}\')\n\n        # In a real implementation, this would:\n        # 1. Send action to appropriate ROS action server\n        # 2. Monitor execution status\n        # 3. Handle failures and timeouts\n        # 4. Update world state based on results\n\n        return True  # Simulate successful execution\n\n    def execute_plan(self, plan: List[Dict[str, Any]]) -> bool:\n        """Execute the complete action plan"""\n        self.plan_execution_status = "executing"\n\n        for i, step in enumerate(plan):\n            self.get_logger().info(f\'Executing step {i+1}/{len(plan)}: {step["action"]}\')\n\n            success = self.execute_plan_step(step)\n\n            if not success:\n                self.get_logger().error(f\'Failed to execute step {i+1}: {step["action"]}\')\n\n                # Try recovery action\n                recovery_action = step.get(\'failure_recovery\')\n                if recovery_action:\n                    self.get_logger().info(f\'Trying recovery: {recovery_action}\')\n                    # Execute recovery logic here\n\n                self.plan_execution_status = "failed"\n                return False\n\n        self.plan_execution_status = "completed"\n        self.get_logger().info(\'Plan execution completed successfully\')\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    cognitive_planner = CognitivePlanningSystem()\n\n    try:\n        rclpy.spin(cognitive_planner)\n    except KeyboardInterrupt:\n        cognitive_planner.get_logger().info(\'Shutting down cognitive planning system\')\n    finally:\n        cognitive_planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),(0,i.jsx)(e.h2,{id:"context-learning-and-adaptation",children:"Context Learning and Adaptation"}),(0,i.jsx)(e.h3,{id:"learning-from-execution-feedback",children:"Learning from Execution Feedback"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_robot_msgs.msg import TaskResult\nimport json\nimport numpy as np\nfrom typing import Dict, List, Tuple\n\nclass ContextLearner(Node):\n    def __init__(self):\n        super().__init__('context_learner')\n\n        # Learning data structures\n        self.execution_history = []  # Store past execution results\n        self.success_patterns = {}   # Patterns that lead to success\n        self.failure_patterns = {}   # Patterns that lead to failure\n        self.adaptation_rules = {}   # Rules for adapting plans\n\n        # Subscriptions\n        self.result_sub = self.create_subscription(\n            TaskResult, '/task_result', self.result_callback, 10)\n\n        # Learning timer\n        self.learning_timer = self.create_timer(10.0, self.perform_learning)\n\n        self.get_logger().info('Context learner initialized')\n\n    def result_callback(self, msg):\n        \"\"\"Process task execution results\"\"\"\n        result_data = {\n            'task_description': msg.task_description,\n            'action_sequence': json.loads(msg.action_sequence),\n            'outcome': msg.outcome,\n            'execution_time': msg.execution_time,\n            'failures': msg.failures,\n            'context': json.loads(msg.context),\n            'timestamp': msg.timestamp.sec\n        }\n\n        self.execution_history.append(result_data)\n\n        # Update success/failure patterns\n        self.update_patterns(result_data)\n\n        self.get_logger().info(f'Task result recorded: {msg.outcome}')\n\n    def update_patterns(self, result: Dict):\n        \"\"\"Update success and failure patterns\"\"\"\n        context_key = self.extract_context_key(result['context'])\n        action_sequence = result['action_sequence']\n        outcome = result['outcome']\n\n        if outcome == 'SUCCESS':\n            if context_key not in self.success_patterns:\n                self.success_patterns[context_key] = []\n            self.success_patterns[context_key].append(action_sequence)\n        else:\n            if context_key not in self.failure_patterns:\n                self.failure_patterns[context_key] = []\n            self.failure_patterns[context_key].append(action_sequence)\n\n    def extract_context_key(self, context: Dict) -> str:\n        \"\"\"Extract key context features for pattern matching\"\"\"\n        # Extract relevant context features\n        features = []\n\n        # Robot state features\n        robot_state = context.get('robot_state', {})\n        features.append(f\"battery_{int(robot_state.get('battery_level', 1.0) * 10)}\")\n\n        # Object features\n        objects = context.get('objects', [])\n        object_types = [obj['type'] for obj in objects]\n        features.extend([f\"obj_{t}\" for t in sorted(set(object_types))])\n\n        # Location features\n        locations = context.get('locations', [])\n        location_types = [loc['type'] for loc in locations]\n        features.extend([f\"loc_{t}\" for t in sorted(set(location_types))])\n\n        return \"_\".join(features)\n\n    def get_adaptation_suggestions(self, current_context: Dict, action_sequence: List[Dict]) -> List[Dict]:\n        \"\"\"Get adaptation suggestions based on learned patterns\"\"\"\n        suggestions = []\n\n        # Find similar contexts\n        current_key = self.extract_context_key(current_context)\n\n        # Check for failure patterns in similar contexts\n        if current_key in self.failure_patterns:\n            recent_failures = self.failure_patterns[current_key][-5:]  # Last 5 failures\n            for failure_seq in recent_failures:\n                # Compare with current sequence to identify problematic patterns\n                problematic_actions = self.compare_sequences(action_sequence, failure_seq)\n                for action_idx, action in problematic_actions:\n                    suggestions.append({\n                        'type': 'avoid',\n                        'action_index': action_idx,\n                        'action': action,\n                        'reason': 'Historical failure pattern detected'\n                    })\n\n        # Check for success patterns\n        if current_key in self.success_patterns:\n            recent_successes = self.success_patterns[current_key][-3:]  # Last 3 successes\n            for success_seq in recent_successes:\n                # Identify successful patterns to replicate\n                successful_actions = self.identify_successful_patterns(action_sequence, success_seq)\n                for action_idx, action in successful_actions:\n                    suggestions.append({\n                        'type': 'emphasize',\n                        'action_index': action_idx,\n                        'action': action,\n                        'reason': 'Historically successful pattern'\n                    })\n\n        return suggestions\n\n    def compare_sequences(self, seq1: List[Dict], seq2: List[Dict]) -> List[Tuple[int, Dict]]:\n        \"\"\"Compare two action sequences to identify differences\"\"\"\n        differences = []\n\n        min_len = min(len(seq1), len(seq2))\n        for i in range(min_len):\n            if seq1[i]['action'] != seq2[i]['action']:\n                differences.append((i, seq1[i]))\n\n        # Add extra actions from longer sequence\n        if len(seq1) > len(seq2):\n            for i in range(min_len, len(seq1)):\n                differences.append((i, seq1[i]))\n\n        return differences\n\n    def identify_successful_patterns(self, current_seq: List[Dict], successful_seq: List[Dict]) -> List[Tuple[int, Dict]]:\n        \"\"\"Identify patterns from successful sequences to apply to current\"\"\"\n        matches = []\n\n        # Simple pattern matching - find similar action sequences\n        for i, action in enumerate(current_seq):\n            for j, success_action in enumerate(successful_seq):\n                if (action['action'] == success_action['action'] and\n                    self.actions_are_similar(action, success_action)):\n                    matches.append((i, action))\n\n        return matches\n\n    def actions_are_similar(self, action1: Dict, action2: Dict) -> bool:\n        \"\"\"Check if two actions are similar enough to be considered a pattern\"\"\"\n        if action1['action'] != action2['action']:\n            return False\n\n        # Compare parameters (simplified)\n        params1 = set(action1.get('parameters', {}).keys())\n        params2 = set(action2.get('parameters', {}).keys())\n\n        # Consider actions similar if they share at least 50% of parameter types\n        common_params = params1.intersection(params2)\n        total_params = params1.union(params2)\n\n        if total_params:\n            return len(common_params) / len(total_params) >= 0.5\n\n        return True\n\n    def perform_learning(self):\n        \"\"\"Perform periodic learning and adaptation\"\"\"\n        if len(self.execution_history) < 10:\n            return  # Need more data for meaningful learning\n\n        # Analyze recent execution patterns\n        recent_results = self.execution_history[-20:]  # Last 20 tasks\n\n        # Calculate success rates for different context types\n        context_success_rates = {}\n        for result in recent_results:\n            ctx_key = self.extract_context_key(result['context'])\n            if ctx_key not in context_success_rates:\n                context_success_rates[ctx_key] = {'success': 0, 'total': 0}\n\n            if result['outcome'] == 'SUCCESS':\n                context_success_rates[ctx_key]['success'] += 1\n            context_success_rates[ctx_key]['total'] += 1\n\n        # Identify problematic contexts\n        for ctx_key, stats in context_success_rates.items():\n            success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0\n            if success_rate < 0.6:  # Less than 60% success rate\n                self.get_logger().warn(f'Low success rate in context {ctx_key}: {success_rate:.2f}')\n\n        self.get_logger().info(f'Learning performed with {len(self.execution_history)} total examples')\n\nclass AdaptiveCognitivePlanner(CognitivePlanningSystem):\n    def __init__(self):\n        super().__init__()\n\n        # Initialize context learner\n        self.context_learner = ContextLearner(self)\n\n    def generate_cognitive_plan(self, task_description: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Generate cognitive plan with adaptation based on learned patterns\"\"\"\n        # First, generate the base plan using LLM\n        base_plan = super().generate_cognitive_plan(task_description, context)\n\n        # Get adaptation suggestions from learned patterns\n        adaptations = self.context_learner.get_adaptation_suggestions(context, base_plan)\n\n        # Apply adaptations to the plan\n        adapted_plan = self.apply_adaptations(base_plan, adaptations)\n\n        return adapted_plan\n\n    def apply_adaptations(self, base_plan: List[Dict], adaptations: List[Dict]) -> List[Dict]:\n        \"\"\"Apply learned adaptations to the base plan\"\"\"\n        adapted_plan = base_plan.copy()\n\n        for adaptation in adaptations:\n            action_idx = adaptation['action_index']\n            adaptation_type = adaptation['type']\n            reason = adaptation['reason']\n\n            if 0 <= action_idx < len(adapted_plan):\n                action = adapted_plan[action_idx]\n\n                if adaptation_type == 'avoid':\n                    # Modify or replace problematic action\n                    self.get_logger().info(f'Adapting action to avoid failure: {reason}')\n                    # Apply appropriate modification based on context\n                    pass\n                elif adaptation_type == 'emphasize':\n                    # Enhance successful action\n                    self.get_logger().info(f'Emphasizing successful action: {reason}')\n                    # Apply appropriate enhancement\n                    pass\n\n        return adapted_plan\n"})}),(0,i.jsx)(e.h2,{id:"planning-validation-and-safety",children:"Planning Validation and Safety"}),(0,i.jsx)(e.h3,{id:"safety-constrained-planning",children:"Safety-Constrained Planning"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_robot_msgs.msg import SafetyConstraints\nimport json\nfrom typing import Dict, List, Any\n\nclass SafetyValidator:\n    def __init__(self, node: Node):\n        self.node = node\n        self.safety_constraints = {\n            'max_velocity': 0.5,  # m/s\n            'max_acceleration': 1.0,  # m/s\xb2\n            'min_obstacle_distance': 0.3,  # meters\n            'max_payload': 5.0,  # kg\n            'joint_limits': {\n                'hip_pitch': (-1.57, 1.57),  # radians\n                'knee_pitch': (-2.0, 0.5),\n                'ankle_pitch': (-0.5, 0.5),\n                'ankle_roll': (-0.3, 0.3)\n            },\n            'balance_constraints': {\n                'max_zmp_deviation': 0.05,  # meters\n                'com_height_range': (0.7, 1.2)  # meters\n            }\n        }\n\n    def validate_action(self, action: Dict[str, Any], current_state: Dict[str, Any]) -> Tuple[bool, List[str]]:\n        \"\"\"Validate a single action against safety constraints\"\"\"\n        violations = []\n\n        action_type = action['action']\n        params = action.get('parameters', {})\n\n        # Check navigation safety\n        if action_type == 'navigate_to':\n            target_pos = params.get('position', {})\n            if target_pos:\n                # Check if target is safe (not too close to obstacles)\n                obstacles = current_state.get('obstacles', [])\n                for obstacle in obstacles:\n                    dist = self.calculate_distance(target_pos, obstacle)\n                    if dist < self.safety_constraints['min_obstacle_distance']:\n                        violations.append(f'Target too close to obstacle: {dist:.2f}m')\n\n        # Check manipulation safety\n        elif action_type == 'pick_up':\n            object_weight = params.get('weight', 0)\n            if object_weight > self.safety_constraints['max_payload']:\n                violations.append(f'Object too heavy: {object_weight}kg > {self.safety_constraints[\"max_payload\"]}kg')\n\n        # Check joint limit safety\n        elif action_type == 'move_joints':\n            joint_positions = params.get('positions', {})\n            for joint_name, position in joint_positions.items():\n                limits = self.safety_constraints['joint_limits'].get(joint_name)\n                if limits:\n                    min_limit, max_limit = limits\n                    if not (min_limit <= position <= max_limit):\n                        violations.append(f'Joint {joint_name} limit violation: {position} rad not in [{min_limit}, {max_limit}]')\n\n        # Check balance constraints\n        elif action_type in ['step', 'move_com']:\n            com_position = current_state.get('center_of_mass', {})\n            zmp_position = current_state.get('zero_moment_point', {})\n\n            # Calculate ZMP deviation\n            if com_position and zmp_position:\n                zmp_dev = self.calculate_zmp_deviation(com_position, zmp_position)\n                if zmp_dev > self.safety_constraints['max_zmp_deviation']:\n                    violations.append(f'ZMP deviation too large: {zmp_dev:.3f}m > {self.safety_constraints[\"max_zmp_deviation\"]}m')\n\n        return len(violations) == 0, violations\n\n    def validate_plan(self, action_sequence: List[Dict[str, Any]], initial_state: Dict[str, Any]) -> Tuple[bool, List[str]]:\n        \"\"\"Validate entire action sequence\"\"\"\n        all_violations = []\n        current_state = initial_state.copy()\n\n        for i, action in enumerate(action_sequence):\n            is_valid, violations = self.validate_action(action, current_state)\n\n            if not is_valid:\n                for violation in violations:\n                    all_violations.append(f'Step {i}: {violation}')\n\n            # Update state for next action validation\n            current_state = self.update_state(current_state, action)\n\n        return len(all_violations) == 0, all_violations\n\n    def calculate_distance(self, pos1: Dict[str, float], pos2: Dict[str, float]) -> float:\n        \"\"\"Calculate Euclidean distance between two positions\"\"\"\n        dx = pos1.get('x', 0) - pos2.get('x', 0)\n        dy = pos1.get('y', 0) - pos2.get('y', 0)\n        dz = pos1.get('z', 0) - pos2.get('z', 0)\n        return (dx*dx + dy*dy + dz*dz)**0.5\n\n    def calculate_zmp_deviation(self, com: Dict[str, float], zmp: Dict[str, float]) -> float:\n        \"\"\"Calculate deviation between center of mass and zero moment point\"\"\"\n        dx = com.get('x', 0) - zmp.get('x', 0)\n        dy = com.get('y', 0) - zmp.get('y', 0)\n        return (dx*dx + dy*dy)**0.5\n\n    def update_state(self, current_state: Dict[str, Any], action: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Update state after action execution (simplified)\"\"\"\n        new_state = current_state.copy()\n\n        # This would be more complex in a real implementation\n        # Update based on action effects\n        if action['action'] == 'navigate_to':\n            new_state['robot_position'] = action['parameters'].get('position', new_state.get('robot_position', {}))\n\n        return new_state\n\nclass SafeCognitivePlanner(AdaptiveCognitivePlanner):\n    def __init__(self):\n        super().__init__()\n        self.safety_validator = SafetyValidator(self)\n\n    def generate_cognitive_plan(self, task_description: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Generate cognitive plan with safety validation\"\"\"\n        # Generate initial plan\n        plan = super().generate_cognitive_plan(task_description, context)\n\n        # Validate the plan\n        is_safe, violations = self.safety_validator.validate_plan(plan, context)\n\n        if not is_safe:\n            self.get_logger().warn(f'Plan has safety violations: {violations}')\n\n            # Attempt to fix the plan\n            safe_plan = self.revise_plan_for_safety(plan, context, violations)\n            return safe_plan\n\n        return plan\n\n    def revise_plan_for_safety(self, plan: List[Dict], context: Dict, violations: List[str]) -> List[Dict]:\n        \"\"\"Revise plan to address safety violations\"\"\"\n        # This would implement plan revision algorithms\n        # For now, return the original plan (in practice, you'd modify it)\n        self.get_logger().info('Revising plan for safety...')\n\n        # Example: If navigation is unsafe, find alternative route\n        for i, action in enumerate(plan):\n            if action['action'] == 'navigate_to':\n                # Check if this action is related to a violation\n                if any('navigate_to' in violation for violation in violations):\n                    # Modify the navigation action to be safer\n                    self.get_logger().info(f'Modifying navigation action {i} for safety')\n                    # Add safety checks, alternative routes, etc.\n\n        return plan\n"})}),(0,i.jsx)(e.h2,{id:"best-practices-for-cognitive-planning",children:"Best Practices for Cognitive Planning"}),(0,i.jsx)(e.h3,{id:"1-robustness-and-error-handling",children:"1. Robustness and Error Handling"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement multiple planning strategies for different scenarios"}),"\n",(0,i.jsx)(e.li,{children:"Design graceful degradation when LLM fails"}),"\n",(0,i.jsx)(e.li,{children:"Include fallback behaviors for common failure modes"}),"\n",(0,i.jsx)(e.li,{children:"Validate plans before execution"}),"\n"]}),(0,i.jsx)(e.h3,{id:"2-performance-optimization",children:"2. Performance Optimization"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Cache frequently used plans and patterns"}),"\n",(0,i.jsx)(e.li,{children:"Use hierarchical planning to reduce complexity"}),"\n",(0,i.jsx)(e.li,{children:"Implement plan refinement rather than complete replanning"}),"\n",(0,i.jsx)(e.li,{children:"Optimize LLM calls with proper prompting"}),"\n"]}),(0,i.jsx)(e.h3,{id:"3-safety-and-reliability",children:"3. Safety and Reliability"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement comprehensive safety validation"}),"\n",(0,i.jsx)(e.li,{children:"Use multiple validation layers (kinematic, dynamic, environmental)"}),"\n",(0,i.jsx)(e.li,{children:"Design for human oversight and intervention"}),"\n",(0,i.jsx)(e.li,{children:"Include uncertainty quantification"}),"\n"]}),(0,i.jsx)(e.h3,{id:"4-learning-and-adaptation",children:"4. Learning and Adaptation"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Continuously update models based on execution results"}),"\n",(0,i.jsx)(e.li,{children:"Learn from both successes and failures"}),"\n",(0,i.jsx)(e.li,{children:"Adapt to changing environments and requirements"}),"\n",(0,i.jsx)(e.li,{children:"Maintain explainability for learned behaviors"}),"\n"]}),(0,i.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),(0,i.jsx)(e.h3,{id:"1-llm-hallucination",children:"1. LLM Hallucination"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use structured outputs (JSON format)"}),"\n",(0,i.jsx)(e.li,{children:"Implement result validation"}),"\n",(0,i.jsx)(e.li,{children:"Cross-reference with known facts"}),"\n",(0,i.jsx)(e.li,{children:"Use multiple LLMs for critical decisions"}),"\n"]}),(0,i.jsx)(e.h3,{id:"2-planning-inconsistency",children:"2. Planning Inconsistency"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Maintain consistent world state representation"}),"\n",(0,i.jsx)(e.li,{children:"Use proper state synchronization"}),"\n",(0,i.jsx)(e.li,{children:"Implement plan monitoring and correction"}),"\n",(0,i.jsx)(e.li,{children:"Design for partial observability"}),"\n"]}),(0,i.jsx)(e.h3,{id:"3-computational-performance",children:"3. Computational Performance"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use local models for real-time planning"}),"\n",(0,i.jsx)(e.li,{children:"Implement plan caching and reuse"}),"\n",(0,i.jsx)(e.li,{children:"Optimize LLM context windows"}),"\n",(0,i.jsx)(e.li,{children:"Consider hierarchical decomposition"}),"\n"]}),(0,i.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement an LLM-based planner using OpenAI or a local model"}),"\n",(0,i.jsx)(e.li,{children:"Integrate with ROS 2 message passing for task coordination"}),"\n",(0,i.jsx)(e.li,{children:"Add context awareness with world state management"}),"\n",(0,i.jsx)(e.li,{children:"Implement safety validation for generated plans"}),"\n",(0,i.jsx)(e.li,{children:"Test with various task scenarios and evaluate performance"}),"\n"]}),(0,i.jsx)(e.h2,{id:"quiz-questions",children:"Quiz Questions"}),(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"What are the key components of a cognitive planning system for humanoid robots?"}),"\n",(0,i.jsx)(e.li,{children:"How does hierarchical task network (HTN) planning improve cognitive reasoning?"}),"\n",(0,i.jsx)(e.li,{children:"What safety considerations are essential when using LLMs for robot planning?"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);