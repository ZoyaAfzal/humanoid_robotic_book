"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[257],{5959:(e,n,i)=>{i.d(n,{A:()=>c});var t=i(6540),s=i(4848);const a=({totalLessons:e,completedLessons:n,currentLesson:i="Current Lesson"})=>{const[a,r]=(0,t.useState)(0);return(0,t.useEffect)(()=>{const i=setTimeout(()=>{r(n/e*100)},300);return()=>clearTimeout(i)},[n,e]),(0,s.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,s.jsxs)("div",{className:"row",children:[(0,s.jsxs)("div",{className:"col col--8",children:[(0,s.jsx)("h3",{children:"Learning Progress"}),(0,s.jsxs)("p",{children:["Currently studying: ",(0,s.jsx)("strong",{children:i})]})]}),(0,s.jsx)("div",{className:"col col--4 text--right",children:(0,s.jsxs)("span",{className:"badge badge--primary",children:[Math.round(a),"% Complete"]})})]}),(0,s.jsxs)("div",{className:"margin-top--md",children:[(0,s.jsx)("div",{className:"progress-indicator",children:(0,s.jsx)("div",{className:"progress-bar",style:{width:`${a}%`,transition:"width 1s ease-in-out"}})}),(0,s.jsx)("div",{className:"margin-top--sm",children:(0,s.jsxs)("small",{children:[n," of ",e," lessons completed"]})})]}),(0,s.jsxs)("div",{className:"margin-top--md",children:[(0,s.jsx)("button",{className:"button button--primary button--sm margin-right--sm",onClick:()=>alert("Lesson marked as completed!"),children:"\u2713 Mark Complete"}),(0,s.jsx)("button",{className:"button button--secondary button--sm",onClick:()=>alert("Lesson bookmarked!"),children:"\u2605 Bookmark"})]})]})},r=({title:e,questions:n})=>{const[i,a]=(0,t.useState)(Array(n.length).fill(null)),[r,o]=(0,t.useState)(!1),[l,d]=(0,t.useState)(!1),c=n.reduce((e,n,t)=>i[t]===n.correctAnswer?e+1:e,0);return(0,s.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,s.jsx)("h3",{children:e}),n.map((e,n)=>(0,s.jsxs)("div",{className:"margin-bottom--md",children:[(0,s.jsxs)("h4",{children:["Question ",n+1,": ",e.question]}),(0,s.jsx)("div",{className:"margin-left--md",children:e.options.map((e,t)=>(0,s.jsx)("div",{className:"margin-bottom--sm",children:(0,s.jsxs)("label",{style:{display:"flex",alignItems:"center"},children:[(0,s.jsx)("input",{type:"radio",name:`question-${n}`,checked:i[n]===t,onChange:()=>((e,n)=>{if(l)return;const t=[...i];t[e]=n,a(t)})(n,t),disabled:l,style:{marginRight:"0.5rem"}}),e]})},t))}),r&&(0,s.jsx)("div",{className:"margin-top--sm padding--sm "+(i[n]===e.correctAnswer?"alert alert--success":"alert alert--danger"),children:i[n]===e.correctAnswer?(0,s.jsx)("p",{children:(0,s.jsx)("strong",{children:"\u2713 Correct!"})}):null!==i[n]?(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"\u2717 Incorrect."})," Correct answer: ",e.options[e.correctAnswer],(0,s.jsx)("br",{}),(0,s.jsx)("small",{children:e.explanation})]}):(0,s.jsx)("p",{children:(0,s.jsx)("small",{children:e.explanation})})})]},e.id)),l?(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"alert alert--success margin-bottom--md",children:[(0,s.jsxs)("h4",{children:["Quiz Results: ",c,"/",n.length," correct"]}),(0,s.jsxs)("p",{children:["You scored ",Math.round(c/n.length*100),"%!"]})]}),(0,s.jsx)("button",{className:"button button--secondary",onClick:()=>{a(Array(n.length).fill(null)),o(!1),d(!1)},children:"Try Again"})]}):(0,s.jsx)("button",{className:"button button--primary",onClick:()=>{d(!0),o(!0)},children:"Submit Quiz"})]})},o=[{id:1,question:"What does ROS stand for?",options:["Robot Operating System","Robotics Operating Software","Remote Operating System","Robotic Operation Suite"],correctAnswer:0,explanation:"ROS stands for Robot Operating System, a flexible framework for writing robot software."},{id:2,question:"Which of the following is a core concept in ROS 2?",options:["Nodes and Topics","Classes and Objects","Functions and Variables","Databases and Tables"],correctAnswer:0,explanation:"Nodes and Topics are fundamental concepts in ROS 2 for communication between different parts of a robot system."}],l=()=>(0,s.jsx)(r,{title:"ROS 2 Fundamentals Quiz",questions:o});var d=i(8774);const c=({children:e,title:n="Lesson",chapter:i=1,lesson:t=1})=>{const r=t>1?t-1:null,o=t<3?t+1:null,c=`/docs/chapter${i}`,m=(e,n)=>{if(1===e){if(1===n)return"spec-kit-plus-workflow";if(2===n)return"physical-ai-embodied-intelligence";if(3===n)return"development-environment-setup"}else if(2===e){if(1===n)return"ros2-architecture";if(2===n)return"humanoid-robot-modeling";if(3===n)return"bridging-ai-agents"}else if(3===e){if(1===n)return"gazebo-environment-setup";if(2===n)return"simulating-physics-collisions";if(3===n)return"sensor-simulation"}else if(4===e){if(1===n)return"isaac-sim-synthetic-data";if(2===n)return"hardware-accelerated-navigation";if(3===n)return"bipedal-path-planning"}else if(5===e){if(1===n)return"voice-to-action";if(2===n)return"cognitive-planning";if(3===n)return"capstone-project-execution"}return"spec-kit-plus-workflow"};m(i,t);return(0,s.jsxs)("div",{className:"interactive-lesson",children:[(0,s.jsxs)("div",{className:"chapter-header margin-bottom--lg",children:[(0,s.jsxs)("span",{className:"chapter-indicator",children:["Chapter ",i," \u2022 Lesson ",t]}),(0,s.jsx)("h1",{children:n})]}),(0,s.jsx)(a,{totalLessons:3,completedLessons:t-1,currentLesson:n}),(0,s.jsx)("div",{className:"lesson-content",children:e}),(0,s.jsxs)("div",{className:"margin-top--xl",children:[(0,s.jsx)("h3",{children:"Knowledge Check"}),(0,s.jsx)(l,{})]}),(0,s.jsx)("div",{className:"margin-top--lg",children:(0,s.jsxs)("div",{className:"row",children:[(0,s.jsx)("div",{className:"col col--6",children:r&&(0,s.jsx)(d.A,{to:`${c}/lesson${r}/${m(i,r)}`,className:"button button--secondary",children:"\u2190 Previous Lesson"})}),(0,s.jsx)("div",{className:"col col--6 text--right",children:o&&(0,s.jsx)(d.A,{to:`${c}/lesson${o}/${m(i,o)}`,className:"button button--primary",children:"Next Lesson \u2192"})})]})}),(0,s.jsxs)("div",{className:"margin-top--lg interactive-controls-container",children:[(0,s.jsx)("button",{className:"personalize-button",onClick:()=>alert("Content personalized!"),children:"\ud83c\udfaf Personalize Learning"}),(0,s.jsx)("button",{className:"translate-button",onClick:()=>alert("Content translated to Urdu!"),children:"\ud83c\udf10 Translate to Urdu"})]})]})}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}},8689:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter4/lesson1/isaac-sim-synthetic-data","title":"isaac-sim-synthetic-data","description":"In this comprehensive lesson, you\'ll explore NVIDIA Isaac Sim, a powerful robotics simulation platform built on NVIDIA Omniverse. Isaac Sim provides photorealistic rendering, advanced physics simulation, and synthetic data generation capabilities essential for training AI models for humanoid robots.","source":"@site/docs/chapter4/lesson1/isaac-sim-synthetic-data.mdx","sourceDirName":"chapter4/lesson1","slug":"/chapter4/lesson1/isaac-sim-synthetic-data","permalink":"/humanoid_robotic_book/docs/chapter4/lesson1/isaac-sim-synthetic-data","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"mySidebar","previous":{"title":"sensor-simulation","permalink":"/humanoid_robotic_book/docs/chapter3/lesson3/sensor-simulation"},"next":{"title":"hardware-accelerated-navigation","permalink":"/humanoid_robotic_book/docs/chapter4/lesson2/hardware-accelerated-navigation"}}');var s=i(4848),a=i(8453),r=i(5959);const o={sidebar_position:1},l="Isaac Sim & Synthetic Data",d={},c=[{value:"Introduction to NVIDIA Isaac Sim",id:"introduction-to-nvidia-isaac-sim",level:2},{value:"Key Features of Isaac Sim",id:"key-features-of-isaac-sim",level:3},{value:"Installing and Setting Up Isaac Sim",id:"installing-and-setting-up-isaac-sim",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Method 1: Omniverse Launcher (Recommended)",id:"method-1-omniverse-launcher-recommended",level:4},{value:"Method 2: Docker Installation",id:"method-2-docker-installation",level:4},{value:"Method 3: Standalone Installation",id:"method-3-standalone-installation",level:4},{value:"Isaac Sim Architecture",id:"isaac-sim-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"USD (Universal Scene Description)",id:"usd-universal-scene-description",level:3},{value:"Creating Humanoid Robot Models in Isaac Sim",id:"creating-humanoid-robot-models-in-isaac-sim",level:2},{value:"Robot Import and Setup",id:"robot-import-and-setup",level:3},{value:"Advanced Robot Configuration",id:"advanced-robot-configuration",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Synthetic Dataset Generation",id:"synthetic-dataset-generation",level:3},{value:"Sensor Simulation in Isaac Sim",id:"sensor-simulation-in-isaac-sim",level:2},{value:"Advanced Camera Simulation",id:"advanced-camera-simulation",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"Integration with AI Training Workflows",id:"integration-with-ai-training-workflows",level:2},{value:"Computer Vision Training Data",id:"computer-vision-training-data",level:3},{value:"Reinforcement Learning Integration",id:"reinforcement-learning-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient Scene Management",id:"efficient-scene-management",level:3},{value:"Best Practices for Isaac Sim",id:"best-practices-for-isaac-sim",level:2},{value:"1. Asset Optimization",id:"1-asset-optimization",level:3},{value:"2. Synthetic Data Quality",id:"2-synthetic-data-quality",level:3},{value:"3. Performance Management",id:"3-performance-management",level:3},{value:"4. Validation and Verification",id:"4-validation-and-verification",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. GPU Memory Issues",id:"1-gpu-memory-issues",level:3},{value:"2. Physics Instability",id:"2-physics-instability",level:3},{value:"3. Rendering Performance",id:"3-rendering-performance",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Quiz Questions",id:"quiz-questions",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(r.A,{title:"Isaac Sim & Synthetic Data",chapter:4,lesson:1,children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"isaac-sim--synthetic-data",children:"Isaac Sim & Synthetic Data"})}),(0,s.jsx)(n.p,{children:"In this comprehensive lesson, you'll explore NVIDIA Isaac Sim, a powerful robotics simulation platform built on NVIDIA Omniverse. Isaac Sim provides photorealistic rendering, advanced physics simulation, and synthetic data generation capabilities essential for training AI models for humanoid robots."}),(0,s.jsx)(n.h2,{id:"introduction-to-nvidia-isaac-sim",children:"Introduction to NVIDIA Isaac Sim"}),(0,s.jsx)(n.p,{children:"NVIDIA Isaac Sim is a comprehensive robotics simulation environment that provides:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Photorealistic Rendering"}),": RTX-powered rendering for realistic sensor simulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Physics"}),": PhysX engine for accurate collision and contact simulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Tools for creating labeled training data for AI models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS/ROS 2 Integration"}),": Seamless integration with ROS and ROS 2 ecosystems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AI Training Environment"}),": Framework for reinforcement learning and computer vision training"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"key-features-of-isaac-sim",children:"Key Features of Isaac Sim"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-Fidelity Graphics"}),": NVIDIA RTX technology for photorealistic rendering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Realistic Physics"}),": NVIDIA PhysX for accurate dynamics simulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Simulation"}),": Advanced camera, LiDAR, IMU, and other sensor models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domain Randomization"}),": Tools for generating diverse training data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement Learning"}),": Integration with RL training frameworks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Digital Twin Creation"}),": Accurate replicas of real robots and environments"]}),"\n"]}),(0,s.jsx)(n.h2,{id:"installing-and-setting-up-isaac-sim",children:"Installing and Setting Up Isaac Sim"}),(0,s.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),(0,s.jsx)(n.p,{children:"Before installing Isaac Sim, ensure your system meets these requirements:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU"}),": NVIDIA RTX 2080 Ti or better (RTX 3080/4080+ recommended)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VRAM"}),": 11GB+ (24GB+ recommended for complex scenes)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU"}),": Multi-core processor (Intel i7 or AMD Ryzen 7+)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RAM"}),": 32GB+ (64GB recommended for large scenes)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OS"}),": Ubuntu 20.04/22.04 LTS or Windows 10/11"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CUDA"}),": CUDA 11.8+ with compatible drivers"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"installation-process",children:"Installation Process"}),(0,s.jsx)(n.p,{children:"Isaac Sim can be installed in several ways:"}),(0,s.jsx)(n.h4,{id:"method-1-omniverse-launcher-recommended",children:"Method 1: Omniverse Launcher (Recommended)"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Download and install Omniverse Launcher from NVIDIA Developer website\n# Launch Isaac Sim through the Omniverse Launcher\n# The launcher handles all dependencies automatically\n"})}),(0,s.jsx)(n.h4,{id:"method-2-docker-installation",children:"Method 2: Docker Installation"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Pull the Isaac Sim Docker image\ndocker pull nvcr.io/nvidia/isaac-sim:latest\n\n# Run Isaac Sim container\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env "DISPLAY" \\\n  --volume="/tmp/.X11-unix:/tmp/.X11-unix:rw" \\\n  --volume="/home/$USER/.Xauthority:/root/.Xauthority:rw" \\\n  --volume="/home/$USER/isaac_sim_data:/isaac_sim_data" \\\n  --privileged \\\n  nvcr.io/nvidia/isaac-sim:latest\n'})}),(0,s.jsx)(n.h4,{id:"method-3-standalone-installation",children:"Method 3: Standalone Installation"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Download Isaac Sim from NVIDIA Developer website\n# Extract and run the installer\nchmod +x install_isaac_sim.sh\n./install_isaac_sim.sh --accept-license\n\n# Launch Isaac Sim\n./isaac-sim.sh\n"})}),(0,s.jsx)(n.h2,{id:"isaac-sim-architecture",children:"Isaac Sim Architecture"}),(0,s.jsx)(n.h3,{id:"core-components",children:"Core Components"}),(0,s.jsx)(n.p,{children:"Isaac Sim consists of several key components:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Omniverse Nucleus"}),": Central server for asset management and collaboration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"USD (Universal Scene Description)"}),": Scene representation format"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kit Framework"}),": Extensible application framework"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PhysX Integration"}),": Advanced physics simulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RTX Renderer"}),": Photorealistic rendering engine"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS/ROS 2 Bridge"}),": Communication with ROS ecosystems"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"usd-universal-scene-description",children:"USD (Universal Scene Description)"}),(0,s.jsx)(n.p,{children:"USD is the foundation of Isaac Sim's scene representation:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example of creating a USD stage programmatically\nimport omni\nfrom pxr import Usd, UsdGeom, Gf\n\n# Create a new USD stage\nstage = Usd.Stage.CreateNew("/path/to/robot_scene.usd")\n\n# Add a robot to the scene\nrobot_prim = stage.DefinePrim("/World/Robot", "Xform")\nrobot_prim.GetReferences().AddReference("/path/to/robot.usd")\n\n# Add lighting\nlight_prim = stage.DefinePrim("/World/Light", "DistantLight")\nlight_prim.GetAttribute("inputs:intensity").Set(3000)\n\n# Add camera\ncamera_prim = stage.DefinePrim("/World/Camera", "Camera")\ncamera_prim.GetAttribute("inputs:clippingRange").Set((0.1, 1000.0))\n\n# Save the stage\nstage.GetRootLayer().Save()\n'})}),(0,s.jsx)(n.h2,{id:"creating-humanoid-robot-models-in-isaac-sim",children:"Creating Humanoid Robot Models in Isaac Sim"}),(0,s.jsx)(n.h3,{id:"robot-import-and-setup",children:"Robot Import and Setup"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example of importing and setting up a humanoid robot in Isaac Sim\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np\n\n# Initialize the world\nworld = World(stage_units_in_meters=1.0)\n\n# Import humanoid robot\nassets_root_path = get_assets_root_path()\nrobot_path = assets_root_path + "/Isaac/Robots/Humanoid/humanoid.usd"\nadd_reference_to_stage(usd_path=robot_path, prim_path="/World/HumanoidRobot")\n\n# Configure robot properties\nrobot_prim = get_prim_at_path("/World/HumanoidRobot")\n# Set up articulation and drive properties for joints\n'})}),(0,s.jsx)(n.h3,{id:"advanced-robot-configuration",children:"Advanced Robot Configuration"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Configure humanoid robot with detailed properties\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import set_targets\nfrom omni.isaac.core.utils.semantics import add_semantics\n\nclass HumanoidRobot:\n    def __init__(self, prim_path: str, name: str):\n        self._prim_path = prim_path\n        self._name = name\n\n        # Add robot to stage\n        add_reference_to_stage(\n            usd_path="/path/to/humanoid_robot.usd",\n            prim_path=prim_path\n        )\n\n        # Create articulation\n        self.articulation = Articulation(prim_path=prim_path)\n\n        # Set up semantic labels for synthetic data\n        self.setup_semantics()\n\n    def setup_semantics(self):\n        # Add semantic labels to different parts of the robot\n        body_parts = [\n            ("/World/HumanoidRobot/torso", "torso"),\n            ("/World/HumanoidRobot/head", "head"),\n            ("/World/HumanoidRobot/left_arm", "left_arm"),\n            ("/World/HumanoidRobot/right_arm", "right_arm"),\n            ("/World/HumanoidRobot/left_leg", "left_leg"),\n            ("/World/HumanoidRobot/right_leg", "right_leg")\n        ]\n\n        for prim_path, label in body_parts:\n            add_semantics(prim_path, "class", label)\n'})}),(0,s.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),(0,s.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),(0,s.jsx)(n.p,{children:"Domain randomization is crucial for creating robust AI models:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import omni\nimport numpy as np\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.stage import get_current_stage\nfrom pxr import Gf, UsdLux, UsdGeom\n\nclass DomainRandomizer:\n    def __init__(self):\n        self.stage = get_current_stage()\n\n    def randomize_lighting(self):\n        """Randomize lighting conditions for synthetic data"""\n        # Get all lights in the scene\n        light_prims = [prim for prim in self.stage.Traverse()\n                      if prim.GetTypeName() == "DistantLight"]\n\n        for light_prim in light_prims:\n            # Randomize light intensity\n            intensity = np.random.uniform(500, 5000)\n            light_prim.GetAttribute("inputs:intensity").Set(intensity)\n\n            # Randomize light direction\n            azimuth = np.random.uniform(0, 2*np.pi)\n            elevation = np.random.uniform(-np.pi/4, np.pi/4)\n\n            # Convert to world coordinates\n            direction = Gf.Vec3f(\n                np.cos(elevation) * np.cos(azimuth),\n                np.cos(elevation) * np.sin(azimuth),\n                np.sin(elevation)\n            )\n            light_prim.GetAttribute("inputs:direction").Set(direction)\n\n    def randomize_materials(self):\n        """Randomize surface materials"""\n        # This would involve changing material properties like\n        # albedo, roughness, metallic properties, etc.\n        pass\n\n    def randomize_camera_parameters(self):\n        """Randomize camera intrinsics and extrinsics"""\n        # Randomize focal length, sensor size, distortion parameters\n        pass\n'})}),(0,s.jsx)(n.h3,{id:"synthetic-dataset-generation",children:"Synthetic Dataset Generation"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import omni\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.core import World\nimport numpy as np\nimport cv2\nimport json\nfrom pathlib import Path\n\nclass SyntheticDatasetGenerator:\n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Initialize synthetic data helper\n        self.sd_helper = SyntheticDataHelper()\n\n        # Create subdirectories for different data types\n        (self.output_dir / "images").mkdir(exist_ok=True)\n        (self.output_dir / "labels").mkdir(exist_ok=True)\n        (self.output_dir / "depth").mkdir(exist_ok=True)\n        (self.output_dir / "seg").mkdir(exist_ok=True)\n\n    def capture_synthetic_data(self, frame_id: int):\n        """Capture synthetic data for training"""\n        # Capture RGB image\n        rgb_data = self.sd_helper.get_rgb()\n        rgb_image = rgb_data.get("rgba")[:, :, :3]\n\n        # Capture segmentation mask\n        seg_data = self.sd_helper.get_semantic_segmentation()\n        seg_mask = seg_data.get("data")\n\n        # Capture depth image\n        depth_data = self.sd_helper.get_depth()\n        depth_image = depth_data.get("depth")\n\n        # Capture pose information\n        pose_data = self.sd_helper.get_pose()\n\n        # Save data\n        cv2.imwrite(f"{self.output_dir}/images/frame_{frame_id:06d}.png",\n                   cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f"{self.output_dir}/seg/frame_{frame_id:06d}.png", seg_mask)\n        np.save(f"{self.output_dir}/depth/frame_{frame_id:06d}.npy", depth_image)\n\n        # Save annotations\n        annotations = {\n            "frame_id": frame_id,\n            "pose": pose_data.tolist() if pose_data is not None else None,\n            "timestamp": omni.timeline.get_timeline().get_current_time(),\n            "objects": self.get_scene_objects()\n        }\n\n        with open(f"{self.output_dir}/labels/frame_{frame_id:06d}.json", \'w\') as f:\n            json.dump(annotations, f)\n\n    def get_scene_objects(self):\n        """Get list of objects in the scene with their properties"""\n        # This would return information about all objects in the scene\n        # including their semantic labels, positions, etc.\n        pass\n\n    def generate_dataset(self, num_frames: int, randomize_scene: bool = True):\n        """Generate a complete synthetic dataset"""\n        for frame_id in range(num_frames):\n            if randomize_scene:\n                # Randomize scene before each capture\n                self.randomize_scene()\n\n            # Capture data\n            self.capture_synthetic_data(frame_id)\n\n            # Step simulation\n            World.instance().step(render=True)\n\n    def randomize_scene(self):\n        """Randomize scene parameters for domain randomization"""\n        # This would call the domain randomization methods\n        pass\n'})}),(0,s.jsx)(n.h2,{id:"sensor-simulation-in-isaac-sim",children:"Sensor Simulation in Isaac Sim"}),(0,s.jsx)(n.h3,{id:"advanced-camera-simulation",children:"Advanced Camera Simulation"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\nimport carb\nimport numpy as np\n\nclass AdvancedCameraSim:\n    def __init__(self, prim_path: str, resolution: tuple = (640, 480)):\n        self.camera = Camera(\n            prim_path=prim_path,\n            frequency=30,\n            resolution=resolution\n        )\n\n        # Configure camera properties\n        self.configure_camera_properties()\n\n    def configure_camera_properties(self):\n        """Configure advanced camera properties"""\n        # Set focal length\n        self.camera.focal_length = 24.0  # mm\n\n        # Set horizontal aperture\n        self.camera.horizontal_aperture = 36.0  # mm\n\n        # Add noise models\n        self.add_noise_models()\n\n    def add_noise_models(self):\n        """Add realistic noise models to camera"""\n        # This would add various noise models like:\n        # - Photon noise\n        # - Read noise\n        # - Fixed pattern noise\n        # - Thermal noise\n        pass\n\n    def capture_with_distortion(self):\n        """Capture images with realistic lens distortion"""\n        # This would simulate lens distortion effects\n        pass\n'})}),(0,s.jsx)(n.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from omni.isaac.sensor import LidarRtx\nimport numpy as np\n\nclass HumanoidLidarSim:\n    def __init__(self, prim_path: str):\n        # Create a 360-degree LiDAR sensor\n        self.lidar = LidarRtx(\n            prim_path=prim_path,\n            translation=np.array([0.0, 0.0, 1.0]),  # Position at head height\n            orientation=np.array([1.0, 0.0, 0.0, 0.0]),  # No rotation\n            config="Example_Rotary_Mechanical_Lidar",\n            rpm=60,  # 60 rotations per minute\n            fov="15x3",\n            horizontal_resolution=0.18,  # 0.18 degree horizontal resolution\n            vertical_resolution=2.0,  # 2.0 degree vertical resolution\n            high_lod=True\n        )\n\n        # Configure noise characteristics\n        self.configure_noise()\n\n    def configure_noise(self):\n        """Configure realistic LiDAR noise"""\n        # Add range noise, intensity noise, etc.\n        pass\n\n    def get_point_cloud(self):\n        """Get point cloud data from LiDAR"""\n        return self.lidar.get_point_cloud_data()\n'})}),(0,s.jsx)(n.h2,{id:"integration-with-ai-training-workflows",children:"Integration with AI Training Workflows"}),(0,s.jsx)(n.h3,{id:"computer-vision-training-data",children:"Computer Vision Training Data"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nfrom pathlib import Path\n\nclass IsaacSimVisionDataset(Dataset):\n    def __init__(self, data_dir: str, transform=None):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n\n        # Get list of all image files\n        self.image_files = list((self.data_dir / \"images\").glob(\"*.png\"))\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_files[idx]\n        image = cv2.imread(str(img_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Load corresponding segmentation mask\n        seg_path = self.data_dir / \"seg\" / img_path.name\n        if seg_path.exists():\n            seg_mask = cv2.imread(str(seg_path), cv2.IMREAD_GRAYSCALE)\n        else:\n            seg_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n\n        # Load depth data\n        depth_path = self.data_dir / \"depth\" / f\"{img_path.stem}.npy\"\n        if depth_path.exists():\n            depth = np.load(str(depth_path))\n        else:\n            depth = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n\n        if self.transform:\n            image = self.transform(image)\n\n        sample = {\n            'image': image,\n            'segmentation': seg_mask,\n            'depth': depth,\n            'filename': img_path.name\n        }\n\n        return sample\n\n# Example training loop using Isaac Sim data\ndef train_model_with_synthetic_data():\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = IsaacSimVisionDataset(\n        data_dir=\"/path/to/synthetic/data\",\n        transform=transform\n    )\n\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n    # Train your model using the synthetic data\n    for epoch in range(10):  # Example training loop\n        for batch_idx, batch in enumerate(dataloader):\n            images = batch['image']\n            segmentation = batch['segmentation']\n\n            # Your training code here\n            pass\n"})}),(0,s.jsx)(n.h3,{id:"reinforcement-learning-integration",children:"Reinforcement Learning Integration"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import gym\nfrom gym import spaces\nimport numpy as np\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\n\nclass IsaacSimHumanoidEnv(gym.Env):\n    def __init__(self):\n        super().__init__()\n\n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1.0, high=1.0, shape=(24,), dtype=np.float32  # 24 DOF humanoid\n        )\n\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(60,), dtype=np.float32  # Example state\n        )\n\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_environment()\n\n    def setup_environment(self):\n        """Setup the simulation environment"""\n        # Add humanoid robot\n        add_reference_to_stage(\n            usd_path="/path/to/humanoid.usd",\n            prim_path="/World/Humanoid"\n        )\n\n        # Add ground plane, obstacles, etc.\n\n    def reset(self):\n        """Reset the environment"""\n        # Reset robot to initial position\n        # Reset simulation\n        self.world.reset()\n\n        # Return initial observation\n        return self.get_observation()\n\n    def step(self, action):\n        """Execute one step in the environment"""\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Step simulation\n        self.world.step(render=True)\n\n        # Get observation\n        observation = self.get_observation()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check if episode is done\n        done = self.is_done()\n\n        info = {}\n\n        return observation, reward, done, info\n\n    def get_observation(self):\n        """Get current observation from the environment"""\n        # This would include robot state, sensor data, etc.\n        pass\n\n    def apply_action(self, action):\n        """Apply action to the robot"""\n        # Convert normalized action to joint commands\n        pass\n\n    def calculate_reward(self):\n        """Calculate reward based on current state"""\n        # Implement reward function for humanoid tasks\n        pass\n\n    def is_done(self):\n        """Check if episode is done"""\n        # Check for termination conditions\n        pass\n'})}),(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),(0,s.jsx)(n.h3,{id:"efficient-scene-management",children:"Efficient Scene Management"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class EfficientSceneManager:\n    def __init__(self):\n        self.active_objects = set()\n        self.object_pool = {}\n\n    def optimize_rendering(self):\n        """Optimize rendering for better performance"""\n        # Use level-of-detail (LOD) for distant objects\n        # Implement occlusion culling\n        # Use efficient lighting models\n        pass\n\n    def batch_operations(self):\n        """Batch operations for better performance"""\n        # Batch similar operations together\n        # Use multi-threading where possible\n        # Optimize USD operations\n        pass\n'})}),(0,s.jsx)(n.h2,{id:"best-practices-for-isaac-sim",children:"Best Practices for Isaac Sim"}),(0,s.jsx)(n.h3,{id:"1-asset-optimization",children:"1. Asset Optimization"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use efficient mesh representations"}),"\n",(0,s.jsx)(n.li,{children:"Optimize textures and materials"}),"\n",(0,s.jsx)(n.li,{children:"Implement level-of-detail (LOD) systems"}),"\n",(0,s.jsx)(n.li,{children:"Use proxy geometries for complex objects"}),"\n"]}),(0,s.jsx)(n.h3,{id:"2-synthetic-data-quality",children:"2. Synthetic Data Quality"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Validate synthetic data against real data"}),"\n",(0,s.jsx)(n.li,{children:"Use appropriate domain randomization"}),"\n",(0,s.jsx)(n.li,{children:"Include realistic noise models"}),"\n",(0,s.jsx)(n.li,{children:"Verify sensor simulation accuracy"}),"\n"]}),(0,s.jsx)(n.h3,{id:"3-performance-management",children:"3. Performance Management"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor GPU and CPU usage"}),"\n",(0,s.jsx)(n.li,{children:"Optimize scene complexity"}),"\n",(0,s.jsx)(n.li,{children:"Use appropriate simulation stepping"}),"\n",(0,s.jsx)(n.li,{children:"Implement efficient data capture pipelines"}),"\n"]}),(0,s.jsx)(n.h3,{id:"4-validation-and-verification",children:"4. Validation and Verification"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Compare simulation results with real robots"}),"\n",(0,s.jsx)(n.li,{children:"Validate sensor models with real hardware"}),"\n",(0,s.jsx)(n.li,{children:"Test control algorithms in both simulation and reality"}),"\n",(0,s.jsx)(n.li,{children:"Document simulation limitations"}),"\n"]}),(0,s.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),(0,s.jsx)(n.h3,{id:"1-gpu-memory-issues",children:"1. GPU Memory Issues"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reduce scene complexity"}),"\n",(0,s.jsx)(n.li,{children:"Use lower resolution textures"}),"\n",(0,s.jsx)(n.li,{children:"Implement object pooling"}),"\n",(0,s.jsx)(n.li,{children:"Optimize USD file sizes"}),"\n"]}),(0,s.jsx)(n.h3,{id:"2-physics-instability",children:"2. Physics Instability"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adjust solver parameters"}),"\n",(0,s.jsx)(n.li,{children:"Verify mass and inertia properties"}),"\n",(0,s.jsx)(n.li,{children:"Check joint limits and dynamics"}),"\n",(0,s.jsx)(n.li,{children:"Reduce simulation timestep"}),"\n"]}),(0,s.jsx)(n.h3,{id:"3-rendering-performance",children:"3. Rendering Performance"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use appropriate quality settings"}),"\n",(0,s.jsx)(n.li,{children:"Implement frustum culling"}),"\n",(0,s.jsx)(n.li,{children:"Optimize lighting calculations"}),"\n",(0,s.jsx)(n.li,{children:"Use efficient material models"}),"\n"]}),(0,s.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install Isaac Sim and verify the installation"}),"\n",(0,s.jsx)(n.li,{children:"Import a humanoid robot model into Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"Configure basic sensors (camera, LiDAR)"}),"\n",(0,s.jsx)(n.li,{children:"Generate a small synthetic dataset with domain randomization"}),"\n",(0,s.jsx)(n.li,{children:"Validate the synthetic data quality"}),"\n"]}),(0,s.jsx)(n.h2,{id:"quiz-questions",children:"Quiz Questions"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the key components of NVIDIA Isaac Sim architecture?"}),"\n",(0,s.jsx)(n.li,{children:"How does domain randomization improve synthetic data quality?"}),"\n",(0,s.jsx)(n.li,{children:"What are the best practices for optimizing Isaac Sim performance?"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);