"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[448],{5959:(e,n,o)=>{o.d(n,{A:()=>l});var t=o(6540),s=o(4848);const i=({totalLessons:e,completedLessons:n,currentLesson:o="Current Lesson"})=>{const[i,r]=(0,t.useState)(0);return(0,t.useEffect)(()=>{const o=setTimeout(()=>{r(n/e*100)},300);return()=>clearTimeout(o)},[n,e]),(0,s.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,s.jsxs)("div",{className:"row",children:[(0,s.jsxs)("div",{className:"col col--8",children:[(0,s.jsx)("h3",{children:"Learning Progress"}),(0,s.jsxs)("p",{children:["Currently studying: ",(0,s.jsx)("strong",{children:o})]})]}),(0,s.jsx)("div",{className:"col col--4 text--right",children:(0,s.jsxs)("span",{className:"badge badge--primary",children:[Math.round(i),"% Complete"]})})]}),(0,s.jsxs)("div",{className:"margin-top--md",children:[(0,s.jsx)("div",{className:"progress-indicator",children:(0,s.jsx)("div",{className:"progress-bar",style:{width:`${i}%`,transition:"width 1s ease-in-out"}})}),(0,s.jsx)("div",{className:"margin-top--sm",children:(0,s.jsxs)("small",{children:[n," of ",e," lessons completed"]})})]}),(0,s.jsxs)("div",{className:"margin-top--md",children:[(0,s.jsx)("button",{className:"button button--primary button--sm margin-right--sm",onClick:()=>alert("Lesson marked as completed!"),children:"\u2713 Mark Complete"}),(0,s.jsx)("button",{className:"button button--secondary button--sm",onClick:()=>alert("Lesson bookmarked!"),children:"\u2605 Bookmark"})]})]})},r=({title:e,questions:n})=>{const[o,i]=(0,t.useState)(Array(n.length).fill(null)),[r,a]=(0,t.useState)(!1),[c,d]=(0,t.useState)(!1),l=n.reduce((e,n,t)=>o[t]===n.correctAnswer?e+1:e,0);return(0,s.jsxs)("div",{className:"card padding--md margin-bottom--lg",children:[(0,s.jsx)("h3",{children:e}),n.map((e,n)=>(0,s.jsxs)("div",{className:"margin-bottom--md",children:[(0,s.jsxs)("h4",{children:["Question ",n+1,": ",e.question]}),(0,s.jsx)("div",{className:"margin-left--md",children:e.options.map((e,t)=>(0,s.jsx)("div",{className:"margin-bottom--sm",children:(0,s.jsxs)("label",{style:{display:"flex",alignItems:"center"},children:[(0,s.jsx)("input",{type:"radio",name:`question-${n}`,checked:o[n]===t,onChange:()=>((e,n)=>{if(c)return;const t=[...o];t[e]=n,i(t)})(n,t),disabled:c,style:{marginRight:"0.5rem"}}),e]})},t))}),r&&(0,s.jsx)("div",{className:"margin-top--sm padding--sm "+(o[n]===e.correctAnswer?"alert alert--success":"alert alert--danger"),children:o[n]===e.correctAnswer?(0,s.jsx)("p",{children:(0,s.jsx)("strong",{children:"\u2713 Correct!"})}):null!==o[n]?(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"\u2717 Incorrect."})," Correct answer: ",e.options[e.correctAnswer],(0,s.jsx)("br",{}),(0,s.jsx)("small",{children:e.explanation})]}):(0,s.jsx)("p",{children:(0,s.jsx)("small",{children:e.explanation})})})]},e.id)),c?(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"alert alert--success margin-bottom--md",children:[(0,s.jsxs)("h4",{children:["Quiz Results: ",l,"/",n.length," correct"]}),(0,s.jsxs)("p",{children:["You scored ",Math.round(l/n.length*100),"%!"]})]}),(0,s.jsx)("button",{className:"button button--secondary",onClick:()=>{i(Array(n.length).fill(null)),a(!1),d(!1)},children:"Try Again"})]}):(0,s.jsx)("button",{className:"button button--primary",onClick:()=>{d(!0),a(!0)},children:"Submit Quiz"})]})},a=[{id:1,question:"What does ROS stand for?",options:["Robot Operating System","Robotics Operating Software","Remote Operating System","Robotic Operation Suite"],correctAnswer:0,explanation:"ROS stands for Robot Operating System, a flexible framework for writing robot software."},{id:2,question:"Which of the following is a core concept in ROS 2?",options:["Nodes and Topics","Classes and Objects","Functions and Variables","Databases and Tables"],correctAnswer:0,explanation:"Nodes and Topics are fundamental concepts in ROS 2 for communication between different parts of a robot system."}],c=()=>(0,s.jsx)(r,{title:"ROS 2 Fundamentals Quiz",questions:a});var d=o(8774);const l=({children:e,title:n="Lesson",chapter:o=1,lesson:t=1})=>{const r=t>1?t-1:null,a=t<3?t+1:null,l=`/docs/chapter${o}`,m=(e,n)=>{if(1===e){if(1===n)return"spec-kit-plus-workflow";if(2===n)return"physical-ai-embodied-intelligence";if(3===n)return"development-environment-setup"}else if(2===e){if(1===n)return"ros2-architecture";if(2===n)return"humanoid-robot-modeling";if(3===n)return"bridging-ai-agents"}else if(3===e){if(1===n)return"gazebo-environment-setup";if(2===n)return"simulating-physics-collisions";if(3===n)return"sensor-simulation"}else if(4===e){if(1===n)return"isaac-sim-synthetic-data";if(2===n)return"hardware-accelerated-navigation";if(3===n)return"bipedal-path-planning"}else if(5===e){if(1===n)return"voice-to-action";if(2===n)return"cognitive-planning";if(3===n)return"capstone-project-execution"}return"spec-kit-plus-workflow"};m(o,t);return(0,s.jsxs)("div",{className:"interactive-lesson",children:[(0,s.jsxs)("div",{className:"chapter-header margin-bottom--lg",children:[(0,s.jsxs)("span",{className:"chapter-indicator",children:["Chapter ",o," \u2022 Lesson ",t]}),(0,s.jsx)("h1",{children:n})]}),(0,s.jsx)(i,{totalLessons:3,completedLessons:t-1,currentLesson:n}),(0,s.jsx)("div",{className:"lesson-content",children:e}),(0,s.jsxs)("div",{className:"margin-top--xl",children:[(0,s.jsx)("h3",{children:"Knowledge Check"}),(0,s.jsx)(c,{})]}),(0,s.jsx)("div",{className:"margin-top--lg",children:(0,s.jsxs)("div",{className:"row",children:[(0,s.jsx)("div",{className:"col col--6",children:r&&(0,s.jsx)(d.A,{to:`${l}/lesson${r}/${m(o,r)}`,className:"button button--secondary",children:"\u2190 Previous Lesson"})}),(0,s.jsx)("div",{className:"col col--6 text--right",children:a&&(0,s.jsx)(d.A,{to:`${l}/lesson${a}/${m(o,a)}`,className:"button button--primary",children:"Next Lesson \u2192"})})]})}),(0,s.jsxs)("div",{className:"margin-top--lg interactive-controls-container",children:[(0,s.jsx)("button",{className:"personalize-button",onClick:()=>alert("Content personalized!"),children:"\ud83c\udfaf Personalize Learning"}),(0,s.jsx)("button",{className:"translate-button",onClick:()=>alert("Content translated to Urdu!"),children:"\ud83c\udf10 Translate to Urdu"})]})]})}},8383:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter5/lesson1/voice-to-action","title":"voice-to-action","description":"In this comprehensive lesson, you\'ll explore how to implement voice-to-action systems for humanoid robots using advanced speech recognition technologies, including OpenAI\'s Whisper model. Voice-to-action systems enable natural human-robot interaction, allowing users to control robots through spoken commands in a conversational manner.","source":"@site/docs/chapter5/lesson1/voice-to-action.mdx","sourceDirName":"chapter5/lesson1","slug":"/chapter5/lesson1/voice-to-action","permalink":"/humanoid_robotic_book/docs/chapter5/lesson1/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"mySidebar","previous":{"title":"bipedal-path-planning","permalink":"/humanoid_robotic_book/docs/chapter4/lesson3/bipedal-path-planning"},"next":{"title":"cognitive-planning","permalink":"/humanoid_robotic_book/docs/chapter5/lesson2/cognitive-planning"}}');var s=o(4848),i=o(8453),r=o(5959);const a={sidebar_position:1},c="Voice-to-Action (Whisper Integration)",d={},l=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"Key Components of Voice-to-Action Systems",id:"key-components-of-voice-to-action-systems",level:3},{value:"Speech Recognition with Whisper",id:"speech-recognition-with-whisper",level:2},{value:"Introduction to Whisper",id:"introduction-to-whisper",level:3},{value:"Whisper Integration in ROS 2",id:"whisper-integration-in-ros-2",level:3},{value:"Natural Language Understanding for Commands",id:"natural-language-understanding-for-commands",level:2},{value:"Command Parser and Intent Classifier",id:"command-parser-and-intent-classifier",level:3},{value:"Voice-to-Action Pipeline Integration",id:"voice-to-action-pipeline-integration",level:2},{value:"Complete Voice-to-Action System",id:"complete-voice-to-action-system",level:3},{value:"Advanced Voice Processing Techniques",id:"advanced-voice-processing-techniques",level:2},{value:"Voice Activity Detection and Noise Reduction",id:"voice-activity-detection-and-noise-reduction",level:3},{value:"Speech Synthesis for Feedback",id:"speech-synthesis-for-feedback",level:2},{value:"Text-to-Speech Integration",id:"text-to-speech-integration",level:3},{value:"Real-time Voice Processing",id:"real-time-voice-processing",level:2},{value:"Optimized Real-time Processing Pipeline",id:"optimized-real-time-processing-pipeline",level:3},{value:"Best Practices for Voice-to-Action Systems",id:"best-practices-for-voice-to-action-systems",level:2},{value:"1. Robustness and Error Handling",id:"1-robustness-and-error-handling",level:3},{value:"2. Performance Optimization",id:"2-performance-optimization",level:3},{value:"3. Privacy and Security",id:"3-privacy-and-security",level:3},{value:"4. User Experience",id:"4-user-experience",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Recognition Accuracy",id:"1-recognition-accuracy",level:3},{value:"2. Latency Issues",id:"2-latency-issues",level:3},{value:"3. Context Understanding",id:"3-context-understanding",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Quiz Questions",id:"quiz-questions",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(r.A,{title:"Voice-to-Action (Whisper Integration)",chapter:5,lesson:1,children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"voice-to-action-whisper-integration",children:"Voice-to-Action (Whisper Integration)"})}),(0,s.jsx)(n.p,{children:"In this comprehensive lesson, you'll explore how to implement voice-to-action systems for humanoid robots using advanced speech recognition technologies, including OpenAI's Whisper model. Voice-to-action systems enable natural human-robot interaction, allowing users to control robots through spoken commands in a conversational manner."}),(0,s.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),(0,s.jsx)(n.p,{children:"Voice-to-action systems bridge the gap between human language and robot actions by:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Converting spoken language to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting user intent"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Mapping"}),": Translating commands to robot behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback Generation"}),": Providing confirmation and status updates"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"key-components-of-voice-to-action-systems",children:"Key Components of Voice-to-Action Systems"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Input Processing"}),": Capturing and preprocessing speech signals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition Engine"}),": Converting speech to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Classification"}),": Understanding command semantics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Execution"}),": Mapping commands to robot behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Feedback"}),": Providing auditory responses"]}),"\n"]}),(0,s.jsx)(n.h2,{id:"speech-recognition-with-whisper",children:"Speech Recognition with Whisper"}),(0,s.jsx)(n.h3,{id:"introduction-to-whisper",children:"Introduction to Whisper"}),(0,s.jsx)(n.p,{children:"OpenAI's Whisper is a state-of-the-art automatic speech recognition (ASR) system trained on a large dataset of diverse audio. It offers several advantages for robotics applications:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Support"}),": Can recognize speech in multiple languages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Performance"}),": Works well in various acoustic conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open Source"}),": Free to use and modify"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multiple Models"}),": Various sizes for different performance/accuracy needs"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"whisper-integration-in-ros-2",children:"Whisper Integration in ROS 2"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport json\n\nclass WhisperSpeechRecognizer(Node):\n    def __init__(self):\n        super().__init__(\'whisper_speech_recognizer\')\n\n        # Initialize Whisper model\n        self.get_logger().info(\'Loading Whisper model...\')\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n        self.get_logger().info(\'Whisper model loaded successfully\')\n\n        # Audio processing parameters\n        self.sample_rate = 16000\n        self.chunk_size = 1024\n        self.audio_queue = queue.Queue()\n        self.recording = False\n\n        # Publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/audio_input\', self.audio_callback, 10)\n        self.text_pub = self.create_publisher(\n            String, \'/speech_text\', 10)\n\n        # Timer for processing audio chunks\n        self.process_timer = self.create_timer(1.0, self.process_audio)\n\n        # Audio stream for direct microphone input (optional)\n        self.audio_stream = None\n        self.setup_audio_stream()\n\n    def setup_audio_stream(self):\n        """Setup audio stream for microphone input"""\n        try:\n            self.audio = pyaudio.PyAudio()\n            self.audio_stream = self.audio.open(\n                format=pyaudio.paInt16,\n                channels=1,\n                rate=self.sample_rate,\n                input=True,\n                frames_per_buffer=self.chunk_size\n            )\n            self.get_logger().info(\'Audio stream initialized\')\n        except Exception as e:\n            self.get_logger().warn(f\'Could not initialize audio stream: {e}\')\n\n    def audio_callback(self, msg):\n        """Handle audio data from ROS topic"""\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n        self.audio_queue.put(audio_data)\n\n    def process_audio(self):\n        """Process accumulated audio data with Whisper"""\n        if self.audio_queue.empty():\n            return\n\n        # Collect audio chunks\n        audio_chunks = []\n        while not self.audio_queue.empty():\n            chunk = self.audio_queue.get()\n            audio_chunks.append(chunk)\n\n        if not audio_chunks:\n            return\n\n        # Concatenate all chunks\n        full_audio = np.concatenate(audio_chunks)\n\n        # Process with Whisper if we have enough audio (at least 1 second)\n        if len(full_audio) >= self.sample_rate:\n            self.recognize_speech(full_audio)\n\n    def recognize_speech(self, audio_data):\n        """Recognize speech using Whisper model"""\n        try:\n            # Convert to the format expected by Whisper\n            audio_tensor = torch.from_numpy(audio_data).float()\n\n            # Transcribe the audio\n            result = self.model.transcribe(audio_tensor.numpy())\n\n            # Extract text and confidence\n            text = result[\'text\'].strip()\n            if text:  # Only publish if we have text\n                self.get_logger().info(f\'Recognized: {text}\')\n\n                # Publish recognized text\n                text_msg = String()\n                text_msg.data = text\n                self.text_pub.publish(text_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in speech recognition: {e}\')\n\n    def record_audio_continuously(self):\n        """Continuously record audio from microphone (separate thread)"""\n        def recording_thread():\n            while self.recording:\n                try:\n                    data = self.audio_stream.read(self.chunk_size, exception_on_overflow=False)\n                    audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n                    self.audio_queue.put(audio_data)\n                except Exception as e:\n                    self.get_logger().error(f\'Error in audio recording: {e}\')\n\n        self.recording = True\n        thread = threading.Thread(target=recording_thread, daemon=True)\n        thread.start()\n\n    def stop_recording(self):\n        """Stop audio recording"""\n        self.recording = False\n        if self.audio_stream:\n            self.audio_stream.stop_stream()\n            self.audio_stream.close()\n        if self.audio:\n            self.audio.terminate()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    recognizer = WhisperSpeechRecognizer()\n\n    try:\n        # Optionally start continuous recording\n        recognizer.record_audio_continuously()\n        rclpy.spin(recognizer)\n    except KeyboardInterrupt:\n        recognizer.get_logger().info(\'Shutting down\')\n    finally:\n        recognizer.stop_recording()\n        recognizer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),(0,s.jsx)(n.h2,{id:"natural-language-understanding-for-commands",children:"Natural Language Understanding for Commands"}),(0,s.jsx)(n.h3,{id:"command-parser-and-intent-classifier",children:"Command Parser and Intent Classifier"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import re\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport spacy\n\nclass CommandType(Enum):\n    MOVE = "move"\n    GREET = "greet"\n    FOLLOW = "follow"\n    STOP = "stop"\n    DANCE = "dance"\n    FETCH = "fetch"\n    SPEAK = "speak"\n    NAVIGATE = "navigate"\n    UNKNOWN = "unknown"\n\n@dataclass\nclass ParsedCommand:\n    command_type: CommandType\n    parameters: Dict[str, str]\n    confidence: float\n    original_text: str\n\nclass CommandParser:\n    def __init__(self):\n        # Load spaCy model for NLP processing\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            self.get_logger().warn("spaCy model not found. Install with: python -m spacy download en_core_web_sm")\n            self.nlp = None\n\n        # Define command patterns\n        self.command_patterns = {\n            CommandType.MOVE: [\n                r"go\\s+(?P<direction>\\w+)",\n                r"move\\s+(?P<direction>\\w+)",\n                r"walk\\s+(?P<direction>\\w+)",\n                r"step\\s+(?P<direction>\\w+)"\n            ],\n            CommandType.GREET: [\n                r"hello",\n                r"hi",\n                r"greet",\n                r"say\\s+hello",\n                r"introduce\\s+yourself"\n            ],\n            CommandType.FOLLOW: [\n                r"follow\\s+(?P<target>\\w+)",\n                r"come\\s+with\\s+me",\n                r"follow\\s+me"\n            ],\n            CommandType.STOP: [\n                r"stop",\n                r"halt",\n                r"freeze",\n                r"stand\\s+still"\n            ],\n            CommandType.DANCE: [\n                r"dance",\n                r"boogie",\n                r"move\\s+to\\s+music"\n            ],\n            CommandType.FETCH: [\n                r"fetch\\s+(?P<object>\\w+)",\n                r"get\\s+(?P<object>\\w+)",\n                r"bring\\s+(?P<object>\\w+)",\n                r"pick\\s+up\\s+(?P<object>\\w+)"\n            ],\n            CommandType.NAVIGATE: [\n                r"go\\s+to\\s+(?P<location>[\\w\\s]+)",\n                r"navigate\\s+to\\s+(?P<location>[\\w\\s]+)",\n                r"move\\s+to\\s+(?P<location>[\\w\\s]+)"\n            ]\n        }\n\n    def parse_command(self, text: str) -> ParsedCommand:\n        """Parse text command and extract intent and parameters"""\n        text_lower = text.lower().strip()\n\n        # Try to match against known patterns\n        for cmd_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    return ParsedCommand(\n                        command_type=cmd_type,\n                        parameters=match.groupdict(),\n                        confidence=0.9,  # High confidence for pattern matches\n                        original_text=text\n                    )\n\n        # If no pattern matches, try NLP-based classification\n        return self.nlp_classify_command(text)\n\n    def nlp_classify_command(self, text: str) -> ParsedCommand:\n        """Use NLP to classify command when pattern matching fails"""\n        if not self.nlp:\n            return ParsedCommand(\n                command_type=CommandType.UNKNOWN,\n                parameters={},\n                confidence=0.0,\n                original_text=text\n            )\n\n        doc = self.nlp(text)\n\n        # Extract entities and intent based on keywords\n        entities = {ent.label_: ent.text for ent in doc.ents}\n        tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n\n        # Simple keyword-based classification\n        if any(word in tokens for word in [\'go\', \'move\', \'walk\', \'step\']):\n            return ParsedCommand(\n                command_type=CommandType.MOVE,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'hello\', \'hi\', \'greet\']):\n            return ParsedCommand(\n                command_type=CommandType.GREET,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'stop\', \'halt\']):\n            return ParsedCommand(\n                command_type=CommandType.STOP,\n                parameters=entities,\n                confidence=0.8,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'follow\', \'come\']):\n            return ParsedCommand(\n                command_type=CommandType.FOLLOW,\n                parameters=entities,\n                confidence=0.75,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'dance\', \'boogie\']):\n            return ParsedCommand(\n                command_type=CommandType.DANCE,\n                parameters=entities,\n                confidence=0.8,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'fetch\', \'get\', \'bring\', \'pick\']):\n            return ParsedCommand(\n                command_type=CommandType.FETCH,\n                parameters=entities,\n                confidence=0.75,\n                original_text=text\n            )\n        elif any(word in tokens for word in [\'navigate\', \'go\', \'move\', \'to\']):\n            return ParsedCommand(\n                command_type=CommandType.NAVIGATE,\n                parameters=entities,\n                confidence=0.7,\n                original_text=text\n            )\n\n        return ParsedCommand(\n            command_type=CommandType.UNKNOWN,\n            parameters=entities,\n            confidence=0.3,\n            original_text=text\n        )\n\n    def extract_parameters(self, text: str, cmd_type: CommandType) -> Dict[str, str]:\n        """Extract specific parameters for command type"""\n        params = {}\n        text_lower = text.lower()\n\n        if cmd_type == CommandType.MOVE:\n            # Extract direction\n            directions = [\'forward\', \'backward\', \'left\', \'right\', \'up\', \'down\']\n            for direction in directions:\n                if direction in text_lower:\n                    params[\'direction\'] = direction\n                    break\n\n        elif cmd_type == CommandType.NAVIGATE:\n            # Extract destination\n            match = re.search(r\'to\\s+(.+?)(?:\\.|$)\', text_lower)\n            if match:\n                params[\'destination\'] = match.group(1).strip()\n\n        elif cmd_type == CommandType.FETCH:\n            # Extract object to fetch\n            match = re.search(r\'(?:fetch|get|bring|pick\\s+up)\\s+(.+?)(?:\\.|$)\', text_lower)\n            if match:\n                params[\'object\'] = match.group(1).strip()\n\n        return params\n'})}),(0,s.jsx)(n.h2,{id:"voice-to-action-pipeline-integration",children:"Voice-to-Action Pipeline Integration"}),(0,s.jsx)(n.h3,{id:"complete-voice-to-action-system",children:"Complete Voice-to-Action System"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom humanoid_robot_msgs.msg import RobotCommand\nfrom builtin_interfaces.msg import Duration\nimport time\n\nclass VoiceToActionSystem(Node):\n    def __init__(self):\n        super().__init__(\'voice_to_action_system\')\n\n        # Initialize components\n        self.command_parser = CommandParser()\n        self.action_executor = ActionExecutor(self)\n\n        # Publishers and subscribers\n        self.speech_sub = self.create_subscription(\n            String, \'/speech_text\', self.speech_callback, 10)\n        self.robot_cmd_pub = self.create_publisher(\n            RobotCommand, \'/robot_command\', 10)\n        self.velocity_pub = self.create_publisher(\n            Twist, \'/cmd_vel\', 10)\n\n        # Feedback publisher\n        self.feedback_pub = self.create_publisher(\n            String, \'/voice_feedback\', 10)\n\n        self.get_logger().info(\'Voice-to-Action system initialized\')\n\n    def speech_callback(self, msg):\n        """Process incoming speech text"""\n        text = msg.data.strip()\n        if not text:\n            return\n\n        self.get_logger().info(f\'Received speech command: {text}\')\n\n        # Parse the command\n        parsed_command = self.command_parser.parse_command(text)\n\n        if parsed_command.command_type != CommandType.UNKNOWN:\n            self.get_logger().info(f\'Parsed command: {parsed_command.command_type.value}, \'\n                                 f\'Parameters: {parsed_command.parameters}, \'\n                                 f\'Confidence: {parsed_command.confidence:.2f}\')\n\n            # Execute the command\n            success = self.action_executor.execute_command(parsed_command)\n\n            # Provide feedback\n            if success:\n                feedback = f"Executing {parsed_command.command_type.value} command"\n            else:\n                feedback = f"Failed to execute {parsed_command.command_type.value} command"\n\n            self.provide_feedback(feedback)\n        else:\n            self.get_logger().warn(f\'Unknown command: {text}\')\n            self.provide_feedback("I didn\'t understand that command")\n\n    def provide_feedback(self, message: str):\n        """Provide auditory or visual feedback"""\n        feedback_msg = String()\n        feedback_msg.data = message\n        self.feedback_pub.publish(feedback_msg)\n\n        self.get_logger().info(f\'Feedback: {message}\')\n\nclass ActionExecutor:\n    def __init__(self, node: Node):\n        self.node = node\n        self.current_behavior = None\n\n    def execute_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute the parsed command"""\n        try:\n            if parsed_command.command_type == CommandType.MOVE:\n                return self.execute_move_command(parsed_command)\n            elif parsed_command.command_type == CommandType.GREET:\n                return self.execute_greet_command(parsed_command)\n            elif parsed_command.command_type == CommandType.FOLLOW:\n                return self.execute_follow_command(parsed_command)\n            elif parsed_command.command_type == CommandType.STOP:\n                return self.execute_stop_command(parsed_command)\n            elif parsed_command.command_type == CommandType.DANCE:\n                return self.execute_dance_command(parsed_command)\n            elif parsed_command.command_type == CommandType.FETCH:\n                return self.execute_fetch_command(parsed_command)\n            elif parsed_command.command_type == CommandType.NAVIGATE:\n                return self.execute_navigate_command(parsed_command)\n            else:\n                self.node.get_logger().warn(f\'Unsupported command type: {parsed_command.command_type}\')\n                return False\n\n        except Exception as e:\n            self.node.get_logger().error(f\'Error executing command: {e}\')\n            return False\n\n    def execute_move_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute move command"""\n        direction = parsed_command.parameters.get(\'direction\', \'forward\')\n        distance = float(parsed_command.parameters.get(\'distance\', 1.0))  # Default 1 meter\n\n        # Create velocity command based on direction\n        twist = Twist()\n\n        if direction in [\'forward\', \'ahead\']:\n            twist.linear.x = 0.5  # m/s\n        elif direction in [\'backward\', \'back\']:\n            twist.linear.x = -0.5\n        elif direction in [\'left\']:\n            twist.linear.y = 0.5\n        elif direction in [\'right\']:\n            twist.linear.y = -0.5\n        elif direction in [\'up\']:\n            twist.linear.z = 0.5\n        elif direction in [\'down\']:\n            twist.linear.z = -0.5\n\n        # Publish command for duration based on distance\n        duration = Duration()\n        duration.sec = int(distance / 0.5)  # Assuming 0.5 m/s speed\n        duration.nanosec = int((distance / 0.5 - duration.sec) * 1e9)\n\n        # Publish the command\n        self.node.velocity_pub.publish(twist)\n\n        # Stop after the specified duration\n        time.sleep(distance / 0.5)\n\n        # Stop the robot\n        stop_twist = Twist()\n        self.node.velocity_pub.publish(stop_twist)\n\n        return True\n\n    def execute_greet_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute greeting command"""\n        # This would trigger a greeting behavior\n        # Could involve speech synthesis, gestures, etc.\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "GREET"\n        robot_cmd.parameters = json.dumps({"greeting_type": "wave_hello"})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_follow_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute follow command"""\n        target = parsed_command.parameters.get(\'target\', \'me\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "FOLLOW"\n        robot_cmd.parameters = json.dumps({"target": target})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_stop_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute stop command"""\n        # Stop any ongoing movement\n        stop_twist = Twist()\n        self.node.velocity_pub.publish(stop_twist)\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "STOP"\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_dance_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute dance command"""\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "DANCE"\n        robot_cmd.parameters = json.dumps({"dance_type": "basic"})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_fetch_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute fetch command"""\n        obj = parsed_command.parameters.get(\'object\', \'item\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "FETCH"\n        robot_cmd.parameters = json.dumps({"object": obj})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\n    def execute_navigate_command(self, parsed_command: ParsedCommand) -> bool:\n        """Execute navigation command"""\n        destination = parsed_command.parameters.get(\'destination\', \'here\')\n\n        robot_cmd = RobotCommand()\n        robot_cmd.command = "NAVIGATE"\n        robot_cmd.parameters = json.dumps({"destination": destination})\n        self.node.robot_cmd_pub.publish(robot_cmd)\n\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_system = VoiceToActionSystem()\n\n    try:\n        rclpy.spin(voice_system)\n    except KeyboardInterrupt:\n        voice_system.get_logger().info(\'Shutting down Voice-to-Action system\')\n    finally:\n        voice_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),(0,s.jsx)(n.h2,{id:"advanced-voice-processing-techniques",children:"Advanced Voice Processing Techniques"}),(0,s.jsx)(n.h3,{id:"voice-activity-detection-and-noise-reduction",children:"Voice Activity Detection and Noise Reduction"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport webrtcvad\nfrom scipy import signal\nimport collections\n\nclass AdvancedVoiceProcessor:\n    def __init__(self):\n        # Initialize WebRTC VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(3)  # Aggressive mode\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Circular buffer for audio frames\n        self.audio_buffer = collections.deque(maxlen=100)  # Store last 100 frames\n\n        # Noise reduction parameters\n        self.noise_threshold = 0.01\n        self.speech_threshold = 0.1\n\n    def detect_voice_activity(self, audio_frame):\n        """Detect if the audio frame contains speech"""\n        # Convert to bytes for WebRTC VAD\n        audio_bytes = (audio_frame * 32767).astype(np.int16).tobytes()\n\n        try:\n            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)\n            return is_speech\n        except:\n            # Fallback: simple energy-based detection\n            energy = np.mean(np.abs(audio_frame))\n            return energy > self.speech_threshold\n\n    def reduce_noise(self, audio_data):\n        """Apply basic noise reduction"""\n        # Apply a simple high-pass filter to remove low-frequency noise\n        b, a = signal.butter(4, 100 / (self.sample_rate / 2), btype=\'high\')\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n\n        # Apply spectral subtraction for noise reduction\n        # (Simplified version - real implementation would be more complex)\n        noise_profile = self.estimate_noise_profile(audio_data)\n        enhanced_audio = self.spectral_subtraction(audio_data, noise_profile)\n\n        return enhanced_audio\n\n    def estimate_noise_profile(self, audio_data):\n        """Estimate noise profile from silent segments"""\n        # For simplicity, assume first 10% of audio is noise\n        noise_segment = audio_data[:len(audio_data)//10]\n        noise_profile = np.mean(np.abs(noise_segment))\n        return noise_profile\n\n    def spectral_subtraction(self, audio_data, noise_profile):\n        """Apply spectral subtraction for noise reduction"""\n        # Convert to frequency domain\n        fft_data = np.fft.fft(audio_data)\n        magnitude = np.abs(fft_data)\n\n        # Subtract noise profile\n        enhanced_magnitude = np.maximum(magnitude - noise_profile, 0)\n\n        # Reconstruct signal\n        phase = np.angle(fft_data)\n        enhanced_fft = enhanced_magnitude * np.exp(1j * phase)\n        enhanced_audio = np.real(np.fft.ifft(enhanced_fft))\n\n        return enhanced_audio\n\n    def preprocess_audio(self, raw_audio):\n        """Complete audio preprocessing pipeline"""\n        # Apply noise reduction\n        denoised_audio = self.reduce_noise(raw_audio)\n\n        # Normalize audio\n        normalized_audio = self.normalize_audio(denoised_audio)\n\n        # Apply voice activity detection\n        is_speech = self.detect_voice_activity(normalized_audio[:self.frame_size])\n\n        return normalized_audio, is_speech\n\n    def normalize_audio(self, audio_data):\n        """Normalize audio to standard range"""\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            normalized = audio_data / max_val\n            # Scale to reasonable range (not too quiet, not clipping)\n            normalized = normalized * 0.8\n        else:\n            normalized = audio_data\n\n        return normalized\n'})}),(0,s.jsx)(n.h2,{id:"speech-synthesis-for-feedback",children:"Speech Synthesis for Feedback"}),(0,s.jsx)(n.h3,{id:"text-to-speech-integration",children:"Text-to-Speech Integration"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pyttsx3\nimport threading\nfrom queue import Queue\n\nclass TextToSpeech:\n    def __init__(self):\n        self.tts_engine = pyttsx3.init()\n\n        # Configure speech properties\n        self.tts_engine.setProperty(\'rate\', 150)  # Words per minute\n        self.tts_engine.setProperty(\'volume\', 0.8)  # Volume level (0.0 to 1.0)\n\n        # Get available voices\n        voices = self.tts_engine.getProperty(\'voices\')\n        if voices:\n            # Use the first available voice (typically female)\n            self.tts_engine.setProperty(\'voice\', voices[0].id)\n\n        # Queue for speech requests\n        self.speech_queue = Queue()\n        self.speaking = False\n\n        # Start speech processing thread\n        self.speech_thread = threading.Thread(target=self.speech_worker, daemon=True)\n        self.speech_thread.start()\n\n    def speak(self, text):\n        """Add text to speech queue"""\n        self.speech_queue.put(text)\n\n    def speech_worker(self):\n        """Process speech requests in separate thread"""\n        while True:\n            try:\n                text = self.speech_queue.get(timeout=1.0)\n                if text:\n                    self.tts_engine.say(text)\n                    self.tts_engine.runAndWait()\n            except:\n                continue  # Timeout, continue loop\n\n    def interrupt_speech(self):\n        """Interrupt current speech"""\n        self.tts_engine.stop()\n\nclass VoiceFeedbackSystem:\n    def __init__(self, node):\n        self.node = node\n        self.tts = TextToSpeech()\n\n        # Predefined responses\n        self.responses = {\n            \'acknowledged\': [\n                "I understand",\n                "Got it",\n                "Okay",\n                "Understood"\n            ],\n            \'executing\': [\n                "I\'m executing that command",\n                "Working on it now",\n                "On it",\n                "Carrying out your request"\n            ],\n            \'completed\': [\n                "Command completed",\n                "Done",\n                "Finished",\n                "All set"\n            ],\n            \'error\': [\n                "I encountered an error",\n                "Something went wrong",\n                "I couldn\'t complete that",\n                "Error occurred"\n            ]\n        }\n\n    def provide_feedback(self, message_type, details=""):\n        """Provide voice feedback based on message type"""\n        import random\n\n        if message_type in self.responses:\n            response = random.choice(self.responses[message_type])\n            if details:\n                response += f". {details}"\n        else:\n            response = str(message_type)\n\n        self.node.get_logger().info(f\'Voice feedback: {response}\')\n        self.tts.speak(response)\n'})}),(0,s.jsx)(n.h2,{id:"real-time-voice-processing",children:"Real-time Voice Processing"}),(0,s.jsx)(n.h3,{id:"optimized-real-time-processing-pipeline",children:"Optimized Real-time Processing Pipeline"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport threading\nimport queue\nimport time\nimport numpy as np\n\nclass RealTimeVoiceProcessor(Node):\n    def __init__(self):\n        super().__init__(\'real_time_voice_processor\')\n\n        # Initialize components\n        self.whisper_model = whisper.load_model("base")\n        self.command_parser = CommandParser()\n        self.voice_feedback = VoiceFeedbackSystem(self)\n\n        # Audio processing\n        self.audio_queue = queue.Queue(maxsize=10)\n        self.processing_lock = threading.Lock()\n        self.last_process_time = time.time()\n\n        # Publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/microphone/audio_raw\', self.audio_callback, 10)\n        self.command_pub = self.create_publisher(\n            String, \'/parsed_commands\', 10)\n\n        # Processing timer\n        self.process_timer = self.create_timer(0.1, self.process_audio_queue)\n\n        # Performance metrics\n        self.processed_audio_count = 0\n        self.start_time = time.time()\n\n    def audio_callback(self, msg):\n        """Handle incoming audio data"""\n        try:\n            # Convert audio data\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to processing queue\n            if not self.audio_queue.full():\n                self.audio_queue.put(audio_data)\n            else:\n                # Queue is full, drop oldest\n                try:\n                    self.audio_queue.get_nowait()\n                    self.audio_queue.put(audio_data)\n                except:\n                    pass  # Queue might be empty now\n        except Exception as e:\n            self.get_logger().error(f\'Error in audio callback: {e}\')\n\n    def process_audio_queue(self):\n        """Process accumulated audio in real-time"""\n        if self.audio_queue.empty():\n            return\n\n        # Collect available audio data\n        audio_chunks = []\n        while not self.audio_queue.empty():\n            try:\n                chunk = self.audio_queue.get_nowait()\n                audio_chunks.append(chunk)\n            except:\n                break\n\n        if not audio_chunks:\n            return\n\n        # Concatenate chunks\n        full_audio = np.concatenate(audio_chunks)\n\n        # Only process if we have enough audio and enough time has passed\n        current_time = time.time()\n        if len(full_audio) >= 8000 and (current_time - self.last_process_time) > 1.0:  # At least 0.5 seconds of audio\n            with self.processing_lock:\n                # Process with Whisper\n                self.process_audio_with_whisper(full_audio)\n                self.last_process_time = current_time\n\n    def process_audio_with_whisper(self, audio_data):\n        """Process audio with Whisper model"""\n        try:\n            # Transcribe audio\n            result = self.whisper_model.transcribe(audio_data)\n            text = result[\'text\'].strip()\n\n            if text:\n                self.get_logger().info(f\'Recognized: {text}\')\n\n                # Parse command\n                parsed_command = self.command_parser.parse_command(text)\n\n                if parsed_command.command_type != CommandType.UNKNOWN:\n                    # Publish parsed command\n                    cmd_msg = String()\n                    cmd_msg.data = f"{parsed_command.command_type.value}:{parsed_command.parameters}"\n                    self.command_pub.publish(cmd_msg)\n\n                    # Provide feedback\n                    self.voice_feedback.provide_feedback(\'acknowledged\')\n\n                    # Execute command (this would be handled by another node)\n                    self.execute_robot_command(parsed_command)\n                else:\n                    self.voice_feedback.provide_feedback(\'error\', "Command not understood")\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in Whisper processing: {e}\')\n\n    def execute_robot_command(self, parsed_command):\n        """Execute robot command (placeholder - would interface with robot control)"""\n        # This would send the command to the robot\'s action execution system\n        self.get_logger().info(f\'Executing command: {parsed_command.command_type.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = RealTimeVoiceProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info(\'Shutting down real-time voice processor\')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),(0,s.jsx)(n.h2,{id:"best-practices-for-voice-to-action-systems",children:"Best Practices for Voice-to-Action Systems"}),(0,s.jsx)(n.h3,{id:"1-robustness-and-error-handling",children:"1. Robustness and Error Handling"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement multiple fallback strategies"}),"\n",(0,s.jsx)(n.li,{children:"Handle network interruptions gracefully"}),"\n",(0,s.jsx)(n.li,{children:"Provide clear error feedback to users"}),"\n",(0,s.jsx)(n.li,{children:"Design for various acoustic environments"}),"\n"]}),(0,s.jsx)(n.h3,{id:"2-performance-optimization",children:"2. Performance Optimization"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use appropriate model sizes for real-time requirements"}),"\n",(0,s.jsx)(n.li,{children:"Implement audio buffering and streaming"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for minimal latency"}),"\n",(0,s.jsx)(n.li,{children:"Consider edge computing for sensitive applications"}),"\n"]}),(0,s.jsx)(n.h3,{id:"3-privacy-and-security",children:"3. Privacy and Security"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement local processing when possible"}),"\n",(0,s.jsx)(n.li,{children:"Secure voice data transmission"}),"\n",(0,s.jsx)(n.li,{children:"Provide clear privacy controls"}),"\n",(0,s.jsx)(n.li,{children:"Consider data retention policies"}),"\n"]}),(0,s.jsx)(n.h3,{id:"4-user-experience",children:"4. User Experience"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design natural, conversational interactions"}),"\n",(0,s.jsx)(n.li,{children:"Provide clear feedback for all actions"}),"\n",(0,s.jsx)(n.li,{children:"Support multiple languages and accents"}),"\n",(0,s.jsx)(n.li,{children:"Implement context-aware command understanding"}),"\n"]}),(0,s.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),(0,s.jsx)(n.h3,{id:"1-recognition-accuracy",children:"1. Recognition Accuracy"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure proper microphone placement and quality"}),"\n",(0,s.jsx)(n.li,{children:"Use noise reduction techniques"}),"\n",(0,s.jsx)(n.li,{children:"Train custom models for domain-specific vocabulary"}),"\n",(0,s.jsx)(n.li,{children:"Implement confidence thresholds"}),"\n"]}),(0,s.jsx)(n.h3,{id:"2-latency-issues",children:"2. Latency Issues"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize model size and complexity"}),"\n",(0,s.jsx)(n.li,{children:"Use streaming recognition when possible"}),"\n",(0,s.jsx)(n.li,{children:"Implement efficient audio buffering"}),"\n",(0,s.jsx)(n.li,{children:"Consider hardware acceleration"}),"\n"]}),(0,s.jsx)(n.h3,{id:"3-context-understanding",children:"3. Context Understanding"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement conversation history tracking"}),"\n",(0,s.jsx)(n.li,{children:"Use context-aware NLP models"}),"\n",(0,s.jsx)(n.li,{children:"Design clear command structures"}),"\n",(0,s.jsx)(n.li,{children:"Provide feedback on understood context"}),"\n"]}),(0,s.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Whisper model for speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Implement a command parser for robot actions"}),"\n",(0,s.jsx)(n.li,{children:"Create a complete voice-to-action pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Test with various voice commands and environments"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for real-time performance and accuracy"}),"\n"]}),(0,s.jsx)(n.h2,{id:"quiz-questions",children:"Quiz Questions"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the key components of a voice-to-action system for humanoid robots?"}),"\n",(0,s.jsx)(n.li,{children:"How does Whisper improve speech recognition accuracy compared to traditional ASR systems?"}),"\n",(0,s.jsx)(n.li,{children:"What are the main challenges in implementing real-time voice processing for robotics?"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);