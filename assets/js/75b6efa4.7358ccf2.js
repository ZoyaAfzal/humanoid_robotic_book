"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[709],{882:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module1/sensors-and-perception","title":"Sensors and Perception in Humanoid Robots","description":"Humanoid robots rely on various sensors to perceive their environment and understand their own state. This perception system is fundamental to their ability to interact safely and effectively with the world around them.","source":"@site/docs/module1/sensors-and-perception.mdx","sourceDirName":"module1","slug":"/module1/sensors-and-perception","permalink":"/humanoid_robotic_book/docs/module1/sensors-and-perception","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Sensors and Perception","sidebar_position":3}}');var o=s(4848),r=s(8453);const t={sidebar_label:"Sensors and Perception",sidebar_position:3},l="Sensors and Perception in Humanoid Robots",a={},c=[{value:"Types of Sensors",id:"types-of-sensors",level:2},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"Exteroceptive Sensors",id:"exteroceptive-sensors",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Computer Vision",id:"computer-vision",level:2},{value:"Auditory Processing",id:"auditory-processing",level:2},{value:"Integration with Control Systems",id:"integration-with-control-systems",level:2}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"sensors-and-perception-in-humanoid-robots",children:"Sensors and Perception in Humanoid Robots"})}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots rely on various sensors to perceive their environment and understand their own state. This perception system is fundamental to their ability to interact safely and effectively with the world around them."}),"\n",(0,o.jsx)(e.h2,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots employ multiple sensor types categorized into two main groups:"}),"\n",(0,o.jsx)(e.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,o.jsx)(e.p,{children:"These sensors measure the robot's internal state:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Joint encoders"}),": Measure joint angles and positions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Inertial Measurement Units (IMUs)"}),": Detect orientation, velocity, and gravitational forces"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Force/Torque sensors"}),": Measure forces applied to joints and limbs"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Current sensors"}),": Monitor motor current to estimate load"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exteroceptive-sensors",children:"Exteroceptive Sensors"}),"\n",(0,o.jsx)(e.p,{children:"These sensors perceive the external environment:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cameras"}),": Visual information for object recognition and navigation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LIDAR"}),": Distance measurement for 3D mapping"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Tactile sensors"}),": Touch and pressure detection"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Microphones"}),": Audio input for speech recognition and environmental sounds"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,o.jsx)(e.p,{children:"Sensor fusion combines data from multiple sensors to create a more accurate and reliable understanding of the environment. Techniques include:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Kalman filters"}),": Optimal estimation combining noisy measurements"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Particle filters"}),": Probabilistic approach for non-linear systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deep learning"}),": Neural networks processing multi-modal sensor data"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"computer-vision",children:"Computer Vision"}),"\n",(0,o.jsx)(e.p,{children:"Computer vision enables humanoid robots to interpret visual information:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object detection"}),": Identifying and locating objects in the environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Face recognition"}),": Identifying and tracking human faces"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gesture recognition"}),": Understanding human gestures and movements"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": Building maps while navigating"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"auditory-processing",children:"Auditory Processing"}),"\n",(0,o.jsx)(e.p,{children:"Auditory systems allow robots to process sound information:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech recognition"}),": Converting spoken language to text"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sound localization"}),": Determining the direction of sound sources"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Noise filtering"}),": Isolating relevant audio from environmental noise"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speaker identification"}),": Recognizing different speakers"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-control-systems",children:"Integration with Control Systems"}),"\n",(0,o.jsx)(e.p,{children:"Sensor data must be processed in real-time to support robot control:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Low-latency processing"}),": Critical for maintaining balance and safety"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robust algorithms"}),": Handling sensor failures and noisy data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Predictive modeling"}),": Anticipating environmental changes"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>t,x:()=>l});var i=s(6540);const o={},r=i.createContext(o);function t(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);